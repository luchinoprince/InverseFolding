{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to implement a new _Dataset_ object and a new _Dataloader_ to deal with the new memory contraints of my computer. I cannot infact just load all the dataset on _RAM_ and the batch it to the model. The main sources for the implementation are this [post](https://teddykoker.com/2020/12/dataloader/) and this [page](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). Actually the latter is just an explanation of what happens under the hood in the former, which will be the one that I actually follow in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\anaconda3\\envs\\IF\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from ioutils import read_fasta, read_encodings\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(q):\n",
    "\n",
    "    embedding = torch.nn.Embedding(q+1, q).requires_grad_(False)\n",
    "    embedding.weight.data.zero_()\n",
    "\n",
    "    embedding.weight[:q, :q] = torch.eye(q)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "class EncodedProteinDataset_new(Dataset):\n",
    "\n",
    "    def __init__(self, msa_folder, encodings_folder, noise=0.0, max_msas=None, max_seqs=None):\n",
    "        #print(\"I am here\")\n",
    "        self.msa_folder = msa_folder\n",
    "        self.encodings_folder = encodings_folder\n",
    "        self.encodings_paths = []\n",
    "        self.msas_paths = []\n",
    "        self.q = None\n",
    "        self.encoding_dim = None\n",
    "        self.noise = noise\n",
    "\n",
    "        # read encoding file names\n",
    "        encoding_files = {s[:7]: s for s in os.listdir(encodings_folder)}\n",
    "\n",
    "        # parse data in folder\n",
    "        for numseq_file in filter(lambda file: file.endswith('pt'), os.listdir(msa_folder)):\n",
    "            #print(numseq_file)\n",
    "                #print(\"Sono qui\")\n",
    "            print(f\"Counter is:{counter}, Counter fail 1:{counter_fail1}, Counter fail 2:{counter_fail2}, length data:{len(self.data)}\", end=\"\\r\")\n",
    "            counter+=1\n",
    "            id = numseq_file[:7]\n",
    "            if id not in encoding_files:\n",
    "                raise ValueError('No encoding file found for MSA file: ' + numseq_file)\n",
    "\n",
    "            numseq_path = os.path.join(msa_folder, numseq_file)\n",
    "            encoding_path = os.path.join(encodings_folder, encoding_files[id])\n",
    "\n",
    "            if not os.path.isfile(encoding_path):\n",
    "                ## This does not give problems\n",
    "                print(\"{} does not exist, skipping {}\".format(encoding_path, numseq_path))\n",
    "                continue\n",
    "\n",
    "            msa = torch.load(numseq_path).type(torch.int) \n",
    "            if msa.shape[1]>=512:\n",
    "                continue\n",
    "\n",
    "            self.encodings_paths.append(encoding_path)\n",
    "            self.msas_paths.append(numseq_path)\n",
    "\n",
    "            if max_msas is not None and len(self.msas_paths) >= max_msas:\n",
    "                break\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.msas_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding_path = self.encodings_paths[idx]\n",
    "        msa_path = self.msas_paths[idx]\n",
    "\n",
    "        msa = torch.load(msa_path).type(torch.int)  ## For later calculation, embedding does not work with uint or with Int8!\n",
    "        encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n",
    "        if self.noise > 0:\n",
    "            encodings = encodings + self.noise*torch.randn(encodings.shape)\n",
    "        if self.encoding_dim is None:\n",
    "                self.encoding_dim = encodings.shape[1]\n",
    "        else:\n",
    "            assert self.encoding_dim == encodings.shape[1], \"Inconsistent encoding dimension\"\n",
    "        \n",
    "        N = msa.shape[1]\n",
    "        if N != encodings.shape[0]: \n",
    "            \"Inconsistent encoding and sequence length for numerical sequence file: \" + numseq_file\n",
    "            \n",
    "        #if N < 512:\n",
    "        return msa, encodings \n",
    "        #else:\n",
    "        #    return ## I return nothing in this case\n",
    "\n",
    "\n",
    "def collate_fn_new(batch, q, batch_msa_size):\n",
    "    \"\"\" Collate function for data loader\n",
    "    \"\"\"\n",
    "    # subsample msas, here batch_msa_size is referred to the number of MSAS the model sees when training the Potts model.\n",
    "    msas = [tuple[0][torch.randint(0, tuple[0].shape[0], (batch_msa_size, )), :] for tuple in batch]\n",
    "\n",
    "    # padding works in the second dimension\n",
    "    msas = [torch.transpose(msa, 1, 0) for msa in msas]\n",
    "\n",
    "    encodings = [tuple[1] for tuple in batch]\n",
    "\n",
    "    msas = pad_sequence(msas, batch_first=True, padding_value=q)\n",
    "    encodings = pad_sequence(encodings, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # permute msa dimension back\n",
    "    msas = torch.transpose(msas, 2, 1)\n",
    "\n",
    "    # the padding mask is the same for all sequences in an msa, so we can just take the first one\n",
    "    padding_mask = msas[:, 0, :] == q\n",
    "\n",
    "    return msas, encodings, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80ecde79b43a34e906e9a2cfb405ccf13abe059aca669f3beedb52d83bf6ee8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
