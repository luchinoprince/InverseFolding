{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(1, \"./../util/\")\n",
    "sys.path.insert(1, \"./../model/\")\n",
    "from encoded_protein_dataset import EncodedProteinDataset#, collate_fn, get_embedding\n",
    "from encoded_protein_dataset_new import get_embedding, EncodedProteinDataset_new, collate_fn_new\n",
    "from pseudolikelihood import get_npll, get_npll_new, get_npll2\n",
    "import torch, torchvision\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from potts_decoder import PottsDecoder\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from functools import partial\n",
    "import biotite.structure\n",
    "from biotite.structure.io import pdbx, pdb\n",
    "from biotite.structure.residues import get_residues\n",
    "from biotite.structure import filter_backbone\n",
    "from biotite.structure import get_chains\n",
    "from biotite.sequence import ProteinSequence\n",
    "from typing import Sequence, Tuple, List\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#import pytorch_warmup as warmup\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "##TURIN HPC\n",
    "#sys.path.insert(1, \"/Data/silva/esm/\")\n",
    "\n",
    "## EUROPA\n",
    "#sys.path.insert(1, \"/home/lucasilva/esm/\")\n",
    "\n",
    "##Lucas computer\n",
    "sys.path.insert(1, \"/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/esm\")\n",
    "import esm\n",
    "#from esm.inverse_folding import util\n",
    "import esm.pretrained as pretrained\n",
    "from ioutils import read_fasta, read_encodings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_mem():\n",
    "    '''\n",
    "    Uses Nvidia's SMI tool to check the current GPU memory usage.\n",
    "    '''\n",
    "    \n",
    "    # Run the command line tool and get the results.\n",
    "    buf = os.popen('nvidia-smi --query-gpu=memory.total,memory.used --format=csv')\n",
    "\n",
    "    # Use csv module to read and parse the result.\n",
    "    reader = csv.reader(buf, delimiter=',')\n",
    "\n",
    "    # Use a pandas table just for nice formatting.\n",
    "    df = pd.DataFrame(reader)\n",
    "\n",
    "    # Use the first row as the column headers.\n",
    "    new_header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = new_header #set the header row as the df header\n",
    "\n",
    "    # Display the formatted table.\n",
    "    #display(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=3\n",
    "N=6\n",
    "q=21\n",
    "M=2\n",
    "\n",
    "encodings = torch.randn((B, N, 512))\n",
    "#encodings = 0.1 * torch.ones((B, N, 512))\n",
    "msas = torch.randint(low=0, high=21, size=(B,M,N))\n",
    "\n",
    "embeddings = get_embedding(q)\n",
    "msas_embedded = embeddings(msas)\n",
    "padding_mask = torch.zeros(size=(B, N),dtype=bool)\n",
    "#padding_mask = torch.randint(low=0, high=2, size=(B,N), dtype=bool)\n",
    "padding_mask_inv = (~padding_mask)\n",
    "\n",
    "input_encoding_dim = 512\n",
    "q=21\n",
    "param_embed_dim = 512\n",
    "n_param_heads = 4\n",
    "d_model = 128\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "device = 0\n",
    "eta = 1e-3\n",
    "dropout = 0.0\n",
    "\n",
    "device=1\n",
    "\n",
    "\n",
    "decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "\n",
    "model_path = 'D:/Data/InverseFolding/Intermediate_Models/parameters_seed_0_batch_size_4_nheads_2_d_128_nparheads_4_dropout_0.1_eta_0.001_update_3000000_noise_0.02'\n",
    "decoder.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "decoder.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_embeddings, fields = decoder.forward_new(encodings, padding_mask)\n",
    "#fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 6, 21])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msas_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 6, 21])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "msas_embedded = msas_embedded.view(B,M,N,q)\n",
    "param_embeddings, fields = decoder.forward_new(encodings, padding_mask)\n",
    "#param_embeddings = 1.0 * torch.ones(param_embeddings.shape)\n",
    "npll2= get_npll2(msas_embedded, param_embeddings, fields, N, q)\n",
    "\n",
    "Q = torch.einsum('bkuia, buhia->bkhia', \n",
    "                    param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1).sum(axis=-1)\n",
    "\n",
    "## Later implement with einsum\n",
    "## First multiplication: (B, N, q, 1, K) * (B, 1, 1, K, K) = (B, N, q, 1, K)\n",
    "## Second multiplication: (B, N, Q, 1, K) * (B, N, Q, K, 1) = (B, N, Q, 1, 1)\n",
    "## Flatten + Sums: (B, N, Q, 1, 1) --> (B, N, Q) ---> (B)\n",
    "aux = torch.matmul(torch.matmul(param_embeddings.permute((0,2,3,1)).unsqueeze(-2), Q.unsqueeze(1).unsqueeze(1)),\n",
    "                        param_embeddings.permute((0,2,3,1)).unsqueeze(-1)).flatten(2,4).sum(axis=-1).sum(axis=-1)\n",
    "## Again we don't need any reegeneering for fields, penalty is wrong\n",
    "penalty2 = eta*(aux - (param_embeddings**2).sum(axis=1).sum(axis=-1).sum(axis=-1) + torch.sum(fields**2))/B\n",
    "npll2 = npll2 * padding_mask_inv.unsqueeze(1)\n",
    "#loss2 = torch.sum(npll2)/(M*torch.sum(padding_mask_inv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.7525e-05, -1.0270e-04, -3.3078e-05], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New penalty implementation, which is much easier\n",
    "\n",
    "Q2 = torch.einsum('bkuia, buhia->bkhia', \n",
    "                    param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1)\n",
    "\n",
    "penalty_new = eta*(torch.sum(torch.sum(Q2,axis=-1)**2) - torch.sum(Q2**2) + torch.sum(fields**2))/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.1347e-05, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0414, 0.0865, 0.0211], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0412, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(param_embeddings**2).sum(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 6, 21])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msas_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "couplings, fields = decoder(encodings, padding_mask)\n",
    "#couplings, fields = decoder._get_params(param_embeddings.reshape(B,4,N,q), N, padding_mask)\n",
    "fields_zero = torch.zeros(fields.shape)\n",
    "\n",
    "npll = get_npll(msas_embedded.reshape(B,M,N*q), couplings, fields, N, q)\n",
    "padding_mask_inv = (~padding_mask)\n",
    "\n",
    "# multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "penalty = eta*(torch.sum(couplings**2) + torch.sum(fields**2))/B\n",
    "#npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "#loss = torch.sum(npll) / (M * torch.sum(padding_mask_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty - penalty_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.0768, 3.0074, 3.0138, 3.0193, 3.0852, 3.0382],\n",
       "         [3.0508, 3.0664, 3.0141, 3.0615, 3.0573, 3.0913]],\n",
       "\n",
       "        [[3.0679, 3.0469, 3.0676, 3.0537, 3.0669, 3.0142],\n",
       "         [3.0711, 2.9423, 3.0888, 3.0356, 3.0901, 3.0915]],\n",
       "\n",
       "        [[2.9672, 3.0382, 2.9788, 3.0473, 3.1011, 3.0270],\n",
       "         [3.0552, 3.0466, 3.0649, 2.9964, 3.0657, 3.0218]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.0310, 3.0544, 3.0617, 3.0262, 3.0674, 3.0377],\n",
       "         [3.0331, 3.0161, 3.0642, 2.9928, 3.0638, 3.0494]],\n",
       "\n",
       "        [[3.0599, 3.0818, 3.0402, 3.0463, 3.0558, 3.0441],\n",
       "         [2.9705, 3.0692, 3.0361, 3.0679, 2.9936, 3.0498]],\n",
       "\n",
       "        [[3.0473, 3.0396, 3.0709, 3.0523, 3.0688, 3.0024],\n",
       "         [3.0891, 3.0557, 2.9659, 3.0878, 3.0186, 2.9629]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npll2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.8326e-05, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let ut test another thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_param_heads = 24\n",
    "N = 100\n",
    "B=2\n",
    "\n",
    "input = torch.randn((B, N, d_model))\n",
    "module = torch.nn.modules.Linear(d_model, d_model*n_param_heads, bias = False)\n",
    "\n",
    "output1 = module(input)\n",
    "\n",
    "P = module.weight.reshape(n_param_heads, d_model, d_model)\n",
    "\n",
    "output2 = (P.unsqueeze(0).unsqueeze(2) @ input.unsqueeze(1).unsqueeze(4)).squeeze()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5280,  0.1547, -0.0513],\n",
       "        [ 0.5419,  0.2455,  0.7675],\n",
       "        [-0.3611, -0.0354,  0.1045]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.reshape(B, N, n_param_heads, d_model).transpose(1,2)[1,0,1:4,1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5280,  0.1547, -0.0513],\n",
       "        [ 0.5419,  0.2455,  0.7675],\n",
       "        [-0.3611, -0.0354,  0.1045]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[1,0,1:4,1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yes they are the same"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us test the MLP module now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP\n",
    "from torch.nn.modules import LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 512\n",
    "hidden_channels = [512]\n",
    "norm_layer = LayerNorm\n",
    "dropout=0.1\n",
    "mlp_layer = MLP(in_channels=512, hidden_channels=hidden_channels, \n",
    "norm_layer=norm_layer, dropout=dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of MLP(\n",
       "  (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (1): Dropout(p=0.1, inplace=True)\n",
       ")>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_layer.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mlp_layer(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 512])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here I test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=4\n",
    "N=512\n",
    "q=21\n",
    "M=16\n",
    "\n",
    "encodings = torch.randn((B, N, 512))\n",
    "#encodings = 0.1 * torch.ones((B, N, 512))\n",
    "msas = torch.randint(low=0, high=21, size=(B,M,N))\n",
    "\n",
    "embeddings = get_embedding(q)\n",
    "#msas_embedded = embeddings(msas)\n",
    "#padding_mask = torch.zeros(size=(B, N),dtype=bool)\n",
    "padding_mask = torch.randint(low=0, high=2, size=(B,N), dtype=bool)\n",
    "padding_mask_inv = (~padding_mask)\n",
    "\n",
    "msas_test = torch.randint(low=0, high=21, size=(B,M,N))\n",
    "encodings_test = torch.randn((B, N, 512))\n",
    "padding_mask_test = torch.randint(low=0, high=2, size=(B,N), dtype=bool)\n",
    "padding_mask_inv_test = (~padding_mask_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOY DATA FOR MEMORY BENCHMARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_loader = [(msas, encodings, padding_mask)]\n",
    "inputs_test_loader = [(msas_test, encodings_test, padding_mask_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is:28 length data:28\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/InverseFolding/train/./../util/encoded_protein_dataset_new.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is:999 length data:9999\r"
     ]
    }
   ],
   "source": [
    "max_msas = 1000\n",
    "msa_dir = \"/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/msas/\"\n",
    "encoding_dir =\"/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/\"\n",
    "\n",
    "\n",
    "## OLD VERSION TOO MEMORY CONSUMING\n",
    "#print(\"I am loading the train and test datasets\")\n",
    "#train_dataset = EncodedProteinDataset(os.path.join(msa_dir, 'train'), encoding_dir, max_msas=max_msas, noise=0.02)          ## Default value of noise used\n",
    "#sequence_test_dataset = EncodedProteinDataset(os.path.join(msa_dir, 'test/sequence'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "#structure_test_dataset = EncodedProteinDataset(os.path.join(msa_dir, 'test/structure'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "#superfamily_test_dataset = EncodedProteinDataset(os.path.join(msa_dir, 'test/superfamily'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "#print(f\"I have loaded the train and test datasets: seq:{len(sequence_test_dataset)}, str:{len(structure_test_dataset)}, super:{len(superfamily_test_dataset)}\")\n",
    "\n",
    "\n",
    "train_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'train'), encoding_dir, noise=0.02, max_msas=max_msas)          ## Default value of noise used\n",
    "sequence_test_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'test/sequence'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "structure_test_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'test/structure'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "superfamily_test_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'test/superfamily'), encoding_dir, noise=0.0, max_msas=max_msas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have loaded the train and test datasets: train:1000, seq:1000, struc:1000, super:1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"I have loaded the train and test datasets: train:{len(train_dataset)}, seq:{len(sequence_test_dataset)}, struc:{len(structure_test_dataset)}, super:{len(superfamily_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_structure_size = 20   ### I think with empty GPU we can hgo up to 10\n",
    "perc_subset_test = 1.0     ## During the training, for every dataset available we select a random 10% of its samples. Now moved to 100% due to noise in evaluation metrics\n",
    "batch_msa_size = 16\n",
    "q = 21                      ##isn't always 21??\n",
    "collate_fn = partial(collate_fn_new, q=q, batch_msa_size=batch_msa_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=True)#, num_workers=3)\n",
    "sequence_test_loader = DataLoader(sequence_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "sampler=RandomSampler(sequence_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(sequence_test_dataset)/10)))#, num_workers=3)\n",
    "\n",
    "structure_test_loader = DataLoader(structure_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "sampler=RandomSampler(structure_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(structure_test_dataset))))#, num_workers=3)\n",
    "\n",
    "superfamily_test_loader = DataLoader(superfamily_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "sampler=RandomSampler(superfamily_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(superfamily_test_dataset))))#, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)/batch_structure_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch size equal to $32$ we do an epoch every roughly 700 update steps, let us check the evaluation metrics every $10$ epochs, hence roughly every $7k$ update steps on the full test dataset(not for the sequence train dataset which is huge). This should give clearer plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(22, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = None\n",
    "embedding = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed = 82477\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "update_steps = 7000                                      ##Usual values are update steps=10^5, test_steps=10^2\n",
    "test_steps = 700\n",
    "bk_iter = False                                                  ## This tells us how ofter we save a model(default values is every ten-thousand updates)\n",
    "#n_epochs = update_steps//(len(train_dataset)//batch_structure_size)   ## the other update steps will be used for \"partial epochs\", I want to save the last complet epoch\n",
    "#print(f\"With update_steps:{update_steps} we will do {n_epochs} full epochs\")\n",
    "\n",
    "input_encoding_dim = 512\n",
    "param_embed_dim = 512\n",
    "n_param_heads = 48\n",
    "d_model = 512 ##old 512\n",
    "n_heads = 8 ## old 8\n",
    "n_layers = 6\n",
    "## Check before running which is the GPU which is free the most and put it as the running device\n",
    "device = 0        ## DON'T SET TO ONE OTHER THAN IN SPECIAL SPECIAL OCCASIONS, VERY NOISYYYYY!\n",
    "dropout = 0.1\n",
    "\n",
    "decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "decoder.to(device)\n",
    "embedding = get_embedding(q)\n",
    "embedding.to(device)\n",
    "\n",
    "#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=update_steps)\n",
    "#warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_new(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    param_embeddings, fields = decoder.forward_new(encodings, padding_mask)\n",
    "    msas_embedded = embedding(msas)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll2(msas_embedded, param_embeddings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    \n",
    "    Q = torch.einsum('bkuia, buhia->bkhia', \n",
    "                param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1)\n",
    "    penalty = eta*(torch.sum(torch.sum(Q,axis=-1)**2) - torch.sum(Q**2) + torch.sum(fields**2))/B\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "\n",
    "def get_loss(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    couplings, fields = decoder(encodings, padding_mask)\n",
    "\n",
    "    # embed and reshape to (B, M, N*q)\n",
    "    msas_embedded = embedding(msas).view(B, M, -1)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll(msas_embedded, couplings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    penalty = eta*(torch.sum(couplings**2) + torch.sum(fields**2))/B\n",
    "\n",
    "    # the padding mask does not contain the msa dimension so we need to multiply by M\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "    #return loss_penalty\n",
    "\n",
    "def get_loss_loader(decoder, loader, eta):\n",
    "\n",
    "    decoder.eval()\n",
    "    losses = 0\n",
    "    iterator = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            iterator+=1\n",
    "            _, npll = get_loss_new(decoder, inputs, eta) \n",
    "            losses+=npll\n",
    "    \n",
    "    return losses/iterator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us load the benchmark model that seem to have plateauded in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk_dir= 'D:/Data/InverseFolding/Intermediate_Models/'\n",
    "fname_par = 'model_04_01_2023.pt'\n",
    "checkpoint = torch.load(os.path.join(bk_dir, fname_par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "decoder.to(device)\n",
    "\n",
    "decoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=1e-3)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-4 ##change the learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=0\n",
    "for inputs in train_loader:\n",
    "    #loss_penalty, train_batch_loss = get_loss_new(decoder, inputs, eta)  \n",
    "    check+=1\n",
    "    if check>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = sequence_test_loss = structure_test_loss=superfamily_test_loss = 2.4\n",
    "update_step=2\n",
    "\n",
    "summary_writer = SummaryWriter()#log_dir=logdir)\n",
    "layout = {\n",
    "    \"metrics\": {\n",
    "        \"loss\": [\"Multiline\", [\"loss/train\", \"loss/sequence\", \"loss/structure\", \"loss/superfamily\"]],}\n",
    "}\n",
    "summary_writer.add_custom_scalars(layout)\n",
    "hyperparams = { 'lr':1e-4, 'eta':1e-4, 'batch_size':batch_structure_size, 'n_param_heads':n_param_heads, 'n_layers':n_layers, \n",
    "                'dropout':dropout, 'param_embed_dim':param_embed_dim, 'n_heads': n_heads}\n",
    "for i in range(50):\n",
    "    update_step=i\n",
    "    summary_writer.add_scalar('loss/train', train_loss+i, update_step)\n",
    "    summary_writer.add_scalar('loss/sequence', sequence_test_loss+i, update_step)\n",
    "    summary_writer.add_scalar('loss/structure', structure_test_loss+i, update_step)\n",
    "    summary_writer.add_scalar('loss/superfamily', superfamily_test_loss+i, update_step)\n",
    "\n",
    "    save_metrics = {'loss/train': 100+i, 'loss/sequence': 50+i, 'loss/structure': 12+i, 'loss/superfamily': 14+i}\n",
    "    summary_writer.add_hparams(hyperparams, save_metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Let us also save the hyperparameters\n",
    "#with summary_writer as w:\n",
    "\n",
    "#save_metrics = {'loss/train': 100, 'loss/sequence': 50, 'loss/structure': 12, 'loss/superfamily': 14}\n",
    "#summary_writer.add_hparams(hyperparams, save_metrics)\n",
    "\n",
    "summary_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.0%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7000 [00:00<?, ?it/s]/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/InverseFolding/train/./../util/encoded_protein_dataset_new.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n",
      "/home/luchinoprince/anaconda3/envs/IF/lib/python3.9/site-packages/torch/nn/modules/transformer.py:544: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n",
      "update_step: 251, epoch: 5.02  train: 11.32, sequence: 4.13, structure: 4.44, superfamily: 4.30:   4%|â–Ž         | 251/7000 [00:42<19:11,  5.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m inputs \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     33\u001b[0m     decoder\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 35\u001b[0m     loss_penalty, train_batch_loss \u001b[39m=\u001b[39m get_loss_new(decoder, inputs, eta)    \u001b[39m## get the current loss for the batch\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()                           \u001b[39m## set previous gradients to 0\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     loss_penalty\u001b[39m.\u001b[39mbackward()                         \u001b[39m## Get gradients\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mget_loss_new\u001b[0;34m(decoder, inputs, eta)\u001b[0m\n\u001b[1;32m     18\u001b[0m penalty \u001b[39m=\u001b[39m eta\u001b[39m*\u001b[39m(torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39msum(Q,axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39msum(Q\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msum(fields\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m))\u001b[39m/\u001b[39mB\n\u001b[1;32m     19\u001b[0m loss_penalty \u001b[39m=\u001b[39m npll_mean \u001b[39m+\u001b[39m penalty\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m loss_penalty, npll_mean\u001b[39m.\u001b[39;49mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Setting up TensorBoard ######\n",
    "#tb_folder = 'dropout_'+ str(args.dropout) + 'noise_' + str(args.noise) +'_new'\n",
    "#tb_folder = 'new_test'\n",
    "#logdir = os.path.join('./runs', tb_folder)\n",
    "#logdir = './runs'\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "summary_writer = SummaryWriter()#log_dir=logdir)\n",
    "layout = {\n",
    "    \"metrics\": {\n",
    "        \"loss\": [\"Multiline\", [\"loss/train\", \"loss/sequence\", \"loss/structure\", \"loss/superfamily\"]],}\n",
    "}\n",
    "summary_writer.add_custom_scalars(layout)\n",
    "\n",
    "## Let us also save the hyperparameters\n",
    "#with summary_writer as w:\n",
    "hyperparams = {'lr':1e-4, 'eta':1e-4, 'batch_size':batch_structure_size, 'batch_msa_size':batch_msa_size, 'n_param_heads':n_param_heads, 'n_layers':n_layers, \n",
    "                'dropout':dropout, 'param_embed_dim':param_embed_dim, 'n_heads': n_heads}\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=1e-4)\n",
    "#scheduler1 = ReduceLROnPlateau(optimizer, 'min', patience=100)\n",
    "#scheduler2 = MultiStepLR(optimizer, milestones=[1000, 2000], gamma = 1/np.sqrt(5))\n",
    "eta = 1e-4\n",
    "start = time.time()\n",
    "with tqdm(total = update_steps) as pbar: ##This is used to have the nice loading bar while training\n",
    "    train_loss = 0\n",
    "    update_step = 0\n",
    "    max_gpu=0\n",
    "    train_batch_losses = []\n",
    "    epoch = 0.0\n",
    "    while update_step < update_steps:\n",
    "        for inputs in train_loader:\n",
    "            decoder.train()\n",
    "\n",
    "            loss_penalty, train_batch_loss = get_loss_new(decoder, inputs, eta)    ## get the current loss for the batch\n",
    "            optimizer.zero_grad()                           ## set previous gradients to 0\n",
    "            loss_penalty.backward()                         ## Get gradients\n",
    "            #clip_grad_norm_(decoder.parameters(), max_norm=2.0, norm_type=2)\n",
    "            loss_penalty.detach()\n",
    "            optimizer.step()                                ## Do a step of GD\n",
    "            update_step += 1                                ## Increase update step (the update steps will count also different batches within the same epoch)\n",
    "            epoch = update_step / len(train_loader)\n",
    "            \n",
    "            train_batch_losses.append(train_batch_loss) ## Here we append the lossess in the different batches within the same epoch\n",
    "\n",
    "            ## We chain two different schedulers, one for the initial stage, and one for the plateau\n",
    "            #scheduler1.step(loss_penalty)\n",
    "            #cheduler2.step()\n",
    "            \n",
    "            ## Better understand the difference between lr_scheduler and warm_scheduler.\n",
    "            #with warmup_scheduler.dampening():\n",
    "            #    lr_scheduler.step()\n",
    "            \n",
    "            ## We want to keep track of the test loss not at every batch, too costrly otherwise. Usually set to once every 100.\n",
    "            #if ((update_step-1) % test_steps == 0) or (update_step == update_steps - 1):\n",
    "            if (update_step  == 1) or (epoch % 10 == 0):\n",
    "                #print(\"I am here\")\n",
    "                #train_loss = np.mean(train_batch_losses)#torch.mean(train_batch_losses)  ##fix this\n",
    "                train_loss = np.mean(train_batch_losses)\n",
    "                summary_writer.add_scalar('loss/train', train_loss, update_step)\n",
    "                #del loss_penalty\n",
    "                #del train_batch_losses\n",
    "                #train_loss=1              \n",
    "                #sequence_test_loss = get_loss_loader(decoder, inputs_test_loader, eta)\n",
    "                sequence_test_loss = get_loss_loader(decoder, sequence_test_loader, eta)\n",
    "                structure_test_loss = get_loss_loader(decoder, structure_test_loader, eta)\n",
    "                superfamily_test_loss = get_loss_loader(decoder, superfamily_test_loader, eta)\n",
    "                #gpu = int((check_gpu_mem().values[device, 1])[1:-4])\n",
    "                #if gpu > max_gpu:\n",
    "                #    max_gpu = gpu\n",
    "                summary_writer.add_scalar('loss/sequence', sequence_test_loss, update_step)\n",
    "                summary_writer.add_scalar('loss/structure', structure_test_loss, update_step)\n",
    "                summary_writer.add_scalar('loss/superfamily', superfamily_test_loss, update_step)\n",
    "\n",
    "                ## UNCOMMENT THIS!\n",
    "                train_batch_losses = []\n",
    "\n",
    "            ##This gives me problem, I had to change the if condition\n",
    "            #pbar.set_description(f'update_step: {update_step}, epoch: {epoch:.2f} train_batch: {train_batch_loss:.2f} train: {train_loss:.2f}, sequence: {sequence_test_loss:.2f}, max_gpu:{max_gpu}')#, GPU total memory: {check_gpu_mem().values[device, 0]}, GPU used: {check_gpu_mem().values[device, 1]}')\n",
    "            pbar.set_description(f'update_step: {update_step}, epoch: {epoch:.2f}  train: {train_loss:.2f}, sequence: {sequence_test_loss:.2f}, structure: {structure_test_loss:.2f}, superfamily: {superfamily_test_loss:.2f}')\n",
    "            pbar.update(1)\n",
    "            \n",
    "print(f\"It took {time.time()-start} seconds\")\n",
    "save_metrics = {'loss/train': train_loss, 'loss/sequence': sequence_test_loss, \n",
    "'loss/structure': structure_test_loss, 'loss/superfamily': superfamily_test_loss}\n",
    "summary_writer.add_hparams(hyperparams, save_metrics)\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder.P.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bk_dir= 'D:/Data/InverseFolding/Intermediate_Models/'\n",
    "fname_par = 'model_17_01_2023.pt'\n",
    "\n",
    "##Arguments of the model, could be inferred\n",
    "args = {}\n",
    "args['n_layers'] = n_layers\n",
    "args['input_encoding_dim'] = input_encoding_dim\n",
    "args['param_embed_dim'] = param_embed_dim\n",
    "args['n_heads'] = n_heads\n",
    "args['n_param_heads'] = n_param_heads\n",
    "args['dropout'] = dropout\n",
    "\n",
    "\n",
    "\n",
    "d = {}\n",
    "d['epoch'] = epoch\n",
    "d['update_step'] = update_step\n",
    "d['batch_size'] = batch_structure_size\n",
    "d['seed'] = seed\n",
    "d['eta'] = eta\n",
    "d['noise'] = 0.02\n",
    "d['args'] = args\n",
    "d['model_state_dict'] = decoder.state_dict()\n",
    "d['optimizer_state_dict'] = optimizer.state_dict()\n",
    "\n",
    "torch.save(d, os.path.join(bk_dir, fname_par))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1543335ba30>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_batch_losses[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.980251363345555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MAX GPU old implementation: roughly $11$ Gbs, time to perform $1000$ update steps is $811$ seconds. \n",
    "- MAX GPU new implementation: roughly $2.3$ Gbs, time to perform $1000$ update steps is $70$ seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef468693205e50fc967ab488061515ab821b0c81a84b9cd6a21a9f585a4dae20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
