{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook I will run the hypertuning for what regards the models trained with pseudolikelihood. To do so we will leverage the package Optuna. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "\n",
    "\n",
    "sys.path.insert(1, \"./../util\")\n",
    "sys.path.insert(1, \"./../model\")\n",
    "from encoded_protein_dataset_new import get_embedding, EncodedProteinDataset_new, collate_fn_new#, dynamic_collate_fn\n",
    "from pseudolikelihood import get_npll2, get_npll_indep\n",
    "import torch, torchvision\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from potts_decoder import PottsDecoder\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from functools import partial\n",
    "import biotite.structure\n",
    "from biotite.structure.io import pdbx, pdb\n",
    "from biotite.structure.residues import get_residues\n",
    "from biotite.structure import filter_backbone\n",
    "from biotite.structure import get_chains\n",
    "from biotite.sequence import ProteinSequence\n",
    "from typing import Sequence, Tuple, List\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "##TURIN HPC\n",
    "#sys.path.insert(1, \"/Data/silva/esm/\")\n",
    "\n",
    "## EUROPA\n",
    "#sys.path.insert(1, \"/home/lucasilva/esm/\")\n",
    "\n",
    "##Lucas computer\n",
    "sys.path.insert(1, \"/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/esm/\")\n",
    "import esm\n",
    "#from esm.inverse_folding import util\n",
    "import esm.pretrained as pretrained\n",
    "from ioutils import read_fasta, read_encodings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "from Bio import SeqIO\n",
    "\n",
    "from dynamic_loader import dynamic_collate_fn, dynamic_cluster\n",
    "import optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is:777 length data:775\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/InverseFolding/train/./../util/encoded_protein_dataset_new.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1vwxh02.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/1vwxh02_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/3o58E00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/3o58E00_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/2wc7A01.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/2wc7A01_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1vwxP00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/1vwxP00_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1vwxG01.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/1vwxG01_train.a3m.pt\n",
      "No encoding file found for MSA file:  1vwxg01_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/3nskB00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/3nskB00_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1dosA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/1dosA00_train.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1ivnA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/train/1ivnA00_train.a3m.pt\n",
      "No encoding file found for MSA file:  3o58e00_train.a3m.pt\n",
      "No encoding file found for MSA file:  1vwxp00_train.a3m.pt\n",
      "No encoding file found for MSA file:  1vwxH02_train.a3m.pt\n",
      "No encoding file found for MSA file:  3o58e00_test.a3m.pt\n",
      "No encoding file found for MSA file:  1vwxg01_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1vwxP00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/1vwxP00_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1dosA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/1dosA00_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/3nskB00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/3nskB00_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/2wc7A01.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/2wc7A01_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1vwxh02.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/1vwxh02_test.a3m.pt\n",
      "No encoding file found for MSA file:  1vwxH02_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1vwxG01.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/1vwxG01_test.a3m.pt\n",
      "No encoding file found for MSA file:  1vwxp00_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/3o58E00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/3o58E00_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1ivnA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/sequence/1ivnA00_test.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/4xhyA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/structure/4xhyA00.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1nthA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/superfamily/1nthA00.a3m.pt\n",
      "/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/1mdcA00.encodings.pt Mismatch in dimension, skipping /home/luchinoprince/split2/test/superfamily/1mdcA00.a3m.pt\n",
      "I have loaded the train and test datasets: train:22463, seq:22396, struc:1364, super:2672\n"
     ]
    }
   ],
   "source": [
    "### IDEA: MSAS PROCEDURE CAN GIVE DIFFERENT OUTPUT SHAPES? ASK\n",
    "max_msas = None\n",
    "#msa_dir = \"/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/msas/\"\n",
    "msa_dir = \"/home/luchinoprince/split2/\"\n",
    "encoding_dir =\"/media/luchinoce/split2/\"\n",
    "encoding_dir =\"/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/structure_encodings/\"\n",
    "\n",
    "train_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'train'), encoding_dir, noise=0.02, max_msas=max_msas)          ## Default value of noise used\n",
    "sequence_test_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'test/sequence'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "structure_test_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'test/structure'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "superfamily_test_dataset = EncodedProteinDataset_new(os.path.join(msa_dir, 'test/superfamily'), encoding_dir, noise=0.0, max_msas=max_msas)\n",
    "print(f\"I have loaded the train and test datasets: train:{len(train_dataset)}, seq:{len(sequence_test_dataset)}, struc:{len(structure_test_dataset)}, super:{len(superfamily_test_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I DON'T CREATE THE DATALOADERS AS THEY WILL DEPEND ON THE HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(22, 21)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = None\n",
    "embedding = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "q=21\n",
    "\n",
    "seed = 24877\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "bk_iter = False                                                  ## This tells us how ofter we save a model(default values is every ten-thousand updates)\n",
    "#n_epochs = update_steps//(len(train_dataset)//batch_structure_size)   ## the other update steps will be used for \"partial epochs\", I want to save the last complet epoch\n",
    "#print(f\"With update_steps:{update_steps} we will do {n_epochs} full epochs\")\n",
    "\n",
    "input_encoding_dim = 512\n",
    "param_embed_dim = 512\n",
    "n_param_heads = 48\n",
    "d_model = 512 ##old 512\n",
    "n_heads = 8 ## old 8\n",
    "n_layers = 6\n",
    "## Check before running which is the GPU which is free the most and put it as the running device\n",
    "device = 0        ## DON'T SET TO ONE OTHER THAN IN SPECIAL SPECIAL OCCASIONS, VERY NOISYYYYY!\n",
    "dropout = 0.1\n",
    "\n",
    "decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "decoder.to(device)\n",
    "embedding = get_embedding(q)\n",
    "embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_new(decoder, inputs, eta_J, eta_h):\n",
    "    \"\"\"eta_J is the multiplicative term in front of the penalized negative pseudo-log-likelihood for the Couplings\n",
    "        if no penalty is proveded for the fields, we put the equal to one another \"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device, non_blocking=True) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    param_embeddings, fields = decoder.forward_new(encodings, padding_mask)\n",
    "    msas_embedded = embedding(msas)\n",
    "    #eta = eta_J\n",
    "    # get npll\n",
    "    npll = get_npll2(msas_embedded, param_embeddings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    \n",
    "    Q = torch.einsum('bkuia, buhia->bkhia', \n",
    "                param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1)\n",
    "    #penalty = eta*(torch.sum(torch.sum(Q,axis=-1)**2) - torch.sum(Q**2) + torch.sum(fields**2))/B\n",
    "    penalty = eta_J/B*(torch.sum(torch.sum(Q,axis=-1)**2) - torch.sum(Q**2)) + eta_h/B*torch.sum(fields**2) \n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "\n",
    "def get_loss_indep(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device, non_blocking=True) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    fields = decoder.forward_indep(encodings, padding_mask)\n",
    "    msas_embedded = embedding(msas)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll_indep(msas_embedded, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    \n",
    "    #Q = torch.einsum('bkuia, buhia->bkhia', \n",
    "    #            param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1)\n",
    "    penalty = eta*(torch.sum(fields**2))/B\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "\n",
    "\n",
    "def get_loss_loader(decoder, loader, eta_J, eta_h):\n",
    "    decoder.eval()\n",
    "    losses = []\n",
    "    #with torch.no_grad():\n",
    "    for effective_batch_size, inputs_packed in loader:\n",
    "        npll_full = 0\n",
    "        for inputs in inputs_packed:\n",
    "            mini_batch_size = inputs[0].shape[0]\n",
    "            #_, npll = get_loss_indep(decoder, inputs, eta_J, eta_h) ## For independent model without couplings\n",
    "            _, npll = get_loss_new(decoder, inputs, eta_J, eta_h)\n",
    "            npll_full += npll*mini_batch_size/effective_batch_size\n",
    "        losses.append(npll_full)\n",
    "            #del inputs\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def train(decoder, inputs_packed, eta_J, eta_h, optimizer, scaler):\n",
    "    effective_batch_size = inputs_packed[0]\n",
    "    loss_penalty_full = 0\n",
    "    train_loss_full = 0\n",
    "    optimizer.zero_grad(set_to_none=True)                           ## set previous gradients to 0\n",
    "    with torch.cuda.amp.autocast():  ## autocasting mixed precision\n",
    "        for inputs in inputs_packed[1]:\n",
    "            mini_batch_size = inputs[0].shape[0]\n",
    "            #loss_penalty, train_batch_loss = get_loss_indep(decoder, inputs, eta_J, eta_h)    ## get the current loss for the batch this is for the independent training\n",
    "            loss_penalty, train_batch_loss = get_loss_new(decoder, inputs, eta_J, eta_h)\n",
    "            loss_penalty = loss_penalty * mini_batch_size/effective_batch_size\n",
    "            train_batch_loss = train_batch_loss * mini_batch_size/effective_batch_size\n",
    "            #loss_penalty.backward()                         ## Get gradients\n",
    "            scaler.scale(loss_penalty).backward()\n",
    "            loss_penalty_full += loss_penalty.detach()\n",
    "            train_loss_full += train_batch_loss\n",
    "    \n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    #optimizer.step()   \n",
    "\n",
    "    return loss_penalty_full, train_loss_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(param, decoder, trial):\n",
    "    batch_msa_size = param['batch_msa_size']\n",
    "    batch_structure_size = 32   ### I think with empty GPU we can go up to 16 easily\n",
    "    perc_subset_test = 1.0     ## During the training, for every dataset available we select a random 10% of its samples\n",
    "    q = 21 ##isn't always 21\n",
    "    \n",
    "    ############################### CREATE DATALOADERS ###############################################################\n",
    "    collate_fn = partial(dynamic_collate_fn, q=q, batch_size=batch_structure_size, batch_msa_size=batch_msa_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=True,\n",
    "    num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "    sequence_test_loader = DataLoader(sequence_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "    num_workers=4, pin_memory=True, sampler=RandomSampler(sequence_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(sequence_test_dataset))))\n",
    "\n",
    "    structure_test_loader = DataLoader(structure_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "    num_workers=4, pin_memory=True, sampler=RandomSampler(structure_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(structure_test_dataset))))\n",
    "\n",
    "    superfamily_test_loader = DataLoader(superfamily_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn, shuffle=False, \n",
    "    num_workers=4, pin_memory=True, sampler=RandomSampler(superfamily_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(superfamily_test_dataset))))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(decoder.parameters(), lr=param['lr'])\n",
    "    eta_J = param['eta_J']\n",
    "    eta_h = param['eta_h']\n",
    "    epoch = 0.0\n",
    "    train_batch_losses = []\n",
    "    update_step = 0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    update_steps = 70000\n",
    "    ## I run 100 epochs\n",
    "    #while epoch < 100:\n",
    "    while update_step < update_steps:\n",
    "        for inputs_packed in train_loader:\n",
    "            update_step += 1                                ## Increase update step (the update steps will count also different batches within the same epoch)\n",
    "            print(f\"I am at epoch: {update_step}\", end=\"\\r\")\n",
    "            decoder.train()\n",
    "            loss_penalty, train_batch_loss = train(decoder, inputs_packed, eta_J, eta_h, optimizer, scaler)\n",
    "            epoch = update_step / len(train_loader)\n",
    "            loss_penalty.detach()\n",
    "            train_batch_losses.append(train_batch_loss)\n",
    "            if (update_step == 1) or (epoch % 5 == 0):\n",
    "                train_loss = np.mean(train_batch_losses)\n",
    "                with torch.no_grad():\n",
    "                    sequence_test_loss = get_loss_loader(decoder, sequence_test_loader, eta_J, eta_h)\n",
    "                    structure_test_loss = get_loss_loader(decoder, structure_test_loader, eta_J, eta_h)\n",
    "                    superfamily_test_loss = get_loss_loader(decoder, superfamily_test_loader, eta_J, eta_h)\n",
    "                #### REPORT TO ALLOW PRUNING, we wait at least 10 EPOCHS to prune ####\n",
    "                if epoch > 6:\n",
    "                    trial.report((sequence_test_loss + structure_test_loss + superfamily_test_loss/3), epoch)\n",
    "                    if trial.should_prune():\n",
    "                        raise optuna.exceptions.TrialPruned()\n",
    "                \n",
    "                train_batch_losses = []\n",
    "\n",
    "    \n",
    "    return sequence_test_loss, structure_test_loss, superfamily_test_loss\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "            'lr' : trial.suggest_uniform('lr', 1e-5, 1e-3),\n",
    "            'eta_J' : trial.suggest_uniform('eta_J', 1e-6, 1e-4),\n",
    "            'eta_h' : trial.suggest_uniform('eta_h', 1e-5, 1e-3),\n",
    "            'batch_msa_size' : trial.suggest_categorical('batch_msa_size', [16, 32, 64])\n",
    "            }\n",
    "    \n",
    "    ## I Recreate otherwise I would risk to start the new training at the previous end.\n",
    "    decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "    decoder.to(device)\n",
    "    embedding = get_embedding(q)\n",
    "    embedding.to(device)\n",
    "    \n",
    "\n",
    "    test1, test2, test3 = train_and_evaluate(param, decoder, trial)\n",
    "\n",
    "    ### This to try to help memory consumption\n",
    "    #decoder = None\n",
    "    #torch.cuda.empty_cache()\n",
    "    return (test1+test2+test3)/3\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-05 19:09:26,001]\u001b[0m A new study created in memory with name: no-name-14abe771-7671-48f1-90c1-6b6f321f79c2\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-05 22:48:55,463]\u001b[0m Trial 0 finished with value: 2.6673463899704113 and parameters: {'lr': 0.0005648914550632188, 'eta_J': 6.895952006905523e-05, 'eta_h': 0.0003256236974166933, 'batch_msa_size': 32}. Best is trial 0 with value: 2.6673463899704113.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 03:35:15,978]\u001b[0m Trial 1 finished with value: 14.512913838442719 and parameters: {'lr': 0.000931126492364323, 'eta_J': 5.1522358244928086e-05, 'eta_h': 0.00032270953769181736, 'batch_msa_size': 64}. Best is trial 0 with value: 2.6673463899704113.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-06 06:35:16,066]\u001b[0m Trial 2 failed with parameters: {'lr': 0.0009026428828152847, 'eta_J': 8.891034988130351e-05, 'eta_h': 0.00018547525344703595, 'batch_msa_size': 16} because of the following error: The value nan is not acceptable..\u001b[0m\n",
      "\u001b[33m[W 2023-05-06 06:35:16,067]\u001b[0m Trial 2 failed with value nan.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 11:22:01,979]\u001b[0m Trial 3 finished with value: 1.7372174856570075 and parameters: {'lr': 0.00019445081758246309, 'eta_J': 7.786339549555277e-05, 'eta_h': 0.0006977184495610499, 'batch_msa_size': 64}. Best is trial 3 with value: 1.7372174856570075.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 16:08:52,068]\u001b[0m Trial 4 finished with value: 1.7150406982027857 and parameters: {'lr': 0.0005048268807375032, 'eta_J': 4.436995535553932e-05, 'eta_h': 0.0008116799044681467, 'batch_msa_size': 64}. Best is trial 4 with value: 1.7150406982027857.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 19:15:41,113]\u001b[0m Trial 5 finished with value: 1.7361042528147237 and parameters: {'lr': 0.00033320999643358945, 'eta_J': 5.6835194069285606e-05, 'eta_h': 0.00043062928429166565, 'batch_msa_size': 16}. Best is trial 4 with value: 1.7150406982027857.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 19:35:26,476]\u001b[0m Trial 6 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 31590\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 21:00:05,835]\u001b[0m Trial 7 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-06 21:19:54,807]\u001b[0m Trial 8 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 02:06:50,662]\u001b[0m Trial 9 finished with value: 1.7036468538304839 and parameters: {'lr': 0.0003813907794209207, 'eta_J': 2.755313918417617e-05, 'eta_h': 0.0006099446295376768, 'batch_msa_size': 64}. Best is trial 9 with value: 1.7036468538304839.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 02:26:37,613]\u001b[0m Trial 10 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 02:49:53,100]\u001b[0m Trial 11 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 03:20:11,443]\u001b[0m Trial 12 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 08:06:57,829]\u001b[0m Trial 13 finished with value: 1.7030524016478141 and parameters: {'lr': 0.0004176929758485403, 'eta_J': 5.427166457222715e-06, 'eta_h': 0.000725082917375717, 'batch_msa_size': 64}. Best is trial 13 with value: 1.7030524016478141.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 12:53:35,899]\u001b[0m Trial 14 finished with value: 1.6797838518972414 and parameters: {'lr': 0.0003624665966690978, 'eta_J': 2.1486061245002986e-06, 'eta_h': 0.0006424942117327073, 'batch_msa_size': 64}. Best is trial 14 with value: 1.6797838518972414.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 13:23:47,961]\u001b[0m Trial 15 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 18:10:43,149]\u001b[0m Trial 16 finished with value: 1.6943064924131714 and parameters: {'lr': 0.00023704241710699475, 'eta_J': 2.493167062989222e-06, 'eta_h': 0.0006453437955003415, 'batch_msa_size': 64}. Best is trial 14 with value: 1.6797838518972414.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 18:41:00,707]\u001b[0m Trial 17 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 70200\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 22:20:56,541]\u001b[0m Trial 18 finished with value: 1.6995136189893474 and parameters: {'lr': 0.0001920510375765753, 'eta_J': 1.7430346570350896e-05, 'eta_h': 0.0004822371832505785, 'batch_msa_size': 32}. Best is trial 14 with value: 1.6797838518972414.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am at epoch: 7020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-07 22:51:10,949]\u001b[0m Trial 19 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=20, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0003624665966690978\n",
      "eta_J: 2.1486061245002986e-06\n",
      "eta_h: 0.0006424942117327073\n",
      "batch_msa_size: 64\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
