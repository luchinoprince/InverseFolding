{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3806d49d",
   "metadata": {
    "id": "comprehensive-respect"
   },
   "source": [
    "In this notebook I will implementing the new decoder module discussed with _Christoph_. From the encodings given by the __ESM__ encoder we begin by attaching two layers of standard _Multi-Head self-attention_ which we then train by minimising the _pseudo-likelihhod_(find package).\n",
    "Remember that from here we assume to be working with tensors, not graphs anymore. Our hope is that all of the relevant information coming from the graph structure has already been encoded in the embeddings of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc54881",
   "metadata": {
    "id": "physical-offense"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.nn import TransformerEncoderLayer, Linear\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import one_hot\n",
    "import pickle\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ec768932",
   "metadata": {
    "id": "e8a2567f-0d56-49a7-a015-d8c250d02fdd"
   },
   "outputs": [],
   "source": [
    "## I think I have to be carefull at the dimensions of the objects\n",
    "class Potts_Decoder(torch.nn.Module):\n",
    "    def __init__(self, n_cat:int, n_layers:int, atten_dim:int, embed_dim:int, n_heads:int, dropout=0.0):\n",
    "        ## We use the init of the superclass Module\n",
    "        super().__init__()\n",
    "        #self.system_size = system_size       ##length of the amino-acid\n",
    "        self.n_cat = n_cat                   ## for proteins this is 21, fix??\n",
    "        self.n_layers = n_layers\n",
    "        self.atten_dim = atten_dim           ## this is the input dimension for the attention layer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.attentions = torch.nn.ModuleList()\n",
    "        self.attentions_activated = []\n",
    "        for _ in range(n_layers):\n",
    "            attention_layer = TransformerEncoderLayer(self.atten_dim, self.n_heads,\n",
    "                                                      dropout=self.dropout, batch_first=True)\n",
    "            self.attentions.append(attention_layer)\n",
    "            self.attentions_activated.append(attention_layer)\n",
    "            self.attentions_activated.append(torch.nn.ReLU())\n",
    "        \n",
    "        self.attentions_sequential = torch.nn.Sequential(*self.attentions_activated)\n",
    "        \n",
    "        ## Maybe add a Linear Layer (usually always done)\n",
    "        self.Linear = Linear(self.atten_dim, self.n_cat*self.embed_dim) ##21 is the number of amino-acids + skip character\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, coupling_mask, padding_mask=None):\n",
    "        ## Padding mask is of silve [B, L_max]\n",
    "        ## I have to provide two masks, one for the coupling matrix, one for the padding\n",
    "        ## For the moment suppose a single protein, the batch_size is in the first dimension\n",
    "        ## Here I suppose batch_first\n",
    "        #print(x.shape)\n",
    "        ## I expect to have something like [1, L, 512]\n",
    "        L = x.shape[1]\n",
    "        ## This now, if we have a batch, it is going to be the maximum batch_size\n",
    "\n",
    "        # (L x q x embed_dim)\n",
    "        x = self.attentions_sequential(x, src_key_padding_mask=padding_mask).squeeze()\n",
    "        ## For the moment I suppose to have a single obs\n",
    "        ## We have to output a matrix, not a vector, we hence do the opposite of CNNs\n",
    "        x = self.Linear(x, src_key_padding_mask=padding_mask).reshape(L, self.n_cat, self.embed_dim) ##Not sure we can do this\n",
    "        \n",
    "        #coupling = x.reshape(L, 1, 21, self.embed_dim) @ torch.transpose(x, 1, 2).reshape(1, L, self.embed_dim, 21)\n",
    "        #coupling = torch.transpose(coupling, 1, 2).reshape(L*21, L*21) ##check whether this is correct, looks yes\n",
    "        coupling = torch.flatten(x, end_dim=1) @ torch.flatten(torch.transpose(torch.transpose(x, 2, 0), 1, 2), \n",
    "                                                               start_dim=1)\n",
    "        #print(coupling.shape)\n",
    "        fields = torch.diag(coupling).reshape(L, self.n_cat)\n",
    "        coupling = coupling * mask     ## element-wise product ##still have to be carefull at padding mask...need to be sure is fine\n",
    "        #coupling = 0.5*(coupling + torch.transpose(coupling,0,1))\n",
    "        \n",
    "        ##This recovers the overall coupling matrix\n",
    "        coupling = coupling + torch.transpose(coupling, 0, 1)\n",
    "        #for i in range(L):\n",
    "        #    coupling[i*self.n_cat : (i+1)*self.n_cat, i*self.n_cat : (i+1)*self.n_cat] = 0.0    \n",
    "        \n",
    "        return fields, coupling, padding_mask  #### we have to return the mask\n",
    "    \n",
    "    \n",
    "    \n",
    "def Pseudo_Likelihood(model, data:Tensor, fields:Tensor, coupling:Tensor, padding_mask:Tensor=None, one_hot_input:bool = False) -> Tensor:\n",
    "    seq = data[0]\n",
    "    if not one_hot_input:\n",
    "        data = one_hot(data, num_classes = model.n_cat).float().view(data.shape[0], -1)\n",
    "    if padding_mask is None:\n",
    "        return torch.mean(torch.logsumexp(energy_diffs(model, data, fields, coupling, one_hot_input=True), dim=-1), axis=-1) \n",
    "    else:\n",
    "        energy_diffs, padding_mask = energy_diffs(model, data, fields, coupling, padding_mask=padding_mask, one_hot_input=True)\n",
    "        return np.sum(torch.logsumexp(energy_diffs, dim=-1), axis=-1)/np.sum(padding_mask, axis=-1)\n",
    "               \n",
    "\n",
    "def Local_Fields(model, data: Tensor, fields:Tensor, coupling:Tensor, one_hot_input: bool = False) -> Tensor:\n",
    "    ndata: int = data.shape[0]\n",
    "    system_size = coupling.shape[0]//model.n_cat\n",
    "    if not one_hot_input:\n",
    "        data = one_hot(data, num_classes = model.n_cat).float().view(data.shape[0], -1)\n",
    "    return data.float() @ coupling + fields.T.reshape(1, -1).expand(ndata, system_size*model.n_cat) \n",
    "\n",
    "def energy_diffs(model, data: Tensor, fields:Tensor, coupling:Tensor, padding_mask:Tensor = None, one_hot_input: bool = False) -> Tensor:\n",
    "\n",
    "    ndata: int = data.shape[0]\n",
    "    system_size = coupling.shape[0]//model.n_cat\n",
    "    if not one_hot_input:\n",
    "        data = one_hot(data, num_classes = model.n_cat).float().view(data.shape[0], -1)\n",
    "    ## We extract the local fields vector\n",
    "    local_fields: Tensor = Local_Fields(model, data, fields, coupling, one_hot_input=True)\n",
    "    \n",
    "    ## torch.mul is element-wise product\n",
    "    local_fields_with_deltas: Tensor = torch.mul(local_fields, data)\n",
    "    local_fields_with_deltas = local_fields_with_deltas.reshape(ndata, system_size, model.n_cat)\n",
    "    local_fields_true: Tensor = torch.sum(local_fields_with_deltas, axis=-1)\n",
    "    local_fields_true = local_fields_true.unsqueeze(-1).expand(ndata, system_size, model.n_cat)\n",
    "    energy_diffs: Tensor = local_fields_true - local_fields.reshape(ndata, system_size, model.n_cat) \n",
    "    if padding_mask is None:\n",
    "        return -1*energy_diffs\n",
    "    else:\n",
    "        energy_diffs = energy_diffs*padding_mask        ##Element-wise product, does it broadcast automatically?\n",
    "        return (-1*energy_diffs, padding_mask)\n",
    "\n",
    "def energy_diffs_position(model, position: int, data: Tensor, fields:Tensor, coupling:Tensor, one_hot_input: bool = False) -> Tensor:\n",
    "\n",
    "    if not one_hot_input:\n",
    "        data = one_hot(data, num_classes = model.n_cat).float().view(data.shape[0], -1)\n",
    "\n",
    "    energy_diffs: Tensor = energy_diffs(model, data, fields, coupling, one_hot_input=True)\n",
    "    ##position is the amino acid position \n",
    "    return energy_diffs[:, position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9dc81547",
   "metadata": {
    "id": "quantitative-boating"
   },
   "outputs": [],
   "source": [
    "def load_obj(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5ac0ddb2",
   "metadata": {
    "id": "right-energy"
   },
   "outputs": [],
   "source": [
    "letter_to_num = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n",
    "                       'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n",
    "                       'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19, \n",
    "                       'N': 2, 'Y': 18, 'M': 12, 'X':20}\n",
    "# 'X' I think is -\n",
    "## got this conversion from GVP github.\n",
    "##This seems very very odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bf6a8f0a",
   "metadata": {
    "id": "11c6a823-d7a5-4317-ba30-322d5ac314ef"
   },
   "outputs": [],
   "source": [
    "class Encoded_Proteins(Dataset):\n",
    "    def __init__(self, path_dir, transform=None, target_transform=None):\n",
    "        self.path_dir = path_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(path_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #protein_path = self.path_dir + '/CATH_430_' + str(idx) \n",
    "        protein_file = os.path.join(self.path_dir, os.listdir(self.path_dir)[idx])\n",
    "        d = load_obj(protein_file)\n",
    "        encoded_protein = d['Encoded_Protein']\n",
    "        #native_seq = d['Native_Seq']\n",
    "        num_seq = d['Num_Seq']\n",
    "        if self.transform:\n",
    "            encoded_protein = self.transform(encoded_protein)\n",
    "        if self.target_transform:\n",
    "            native_seq = self.target_transform(native_seq)\n",
    "        return encoded_protein, num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fce6a5f9",
   "metadata": {
    "id": "2f885dac-9721-4975-9fc6-4738a248ffd3"
   },
   "outputs": [],
   "source": [
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    Fill in\n",
    "    \"\"\"\n",
    "    #item = batch[0]\n",
    "    #print(item)\n",
    "    \n",
    "    data_x = [item[0] for item in batch]\n",
    "    data_y = [item[1] for item in batch]\n",
    "    # each element is of size (1, h*, w*). where (h*, w*) changes from mask to another.\n",
    "    ## data_y is already numerical\n",
    "    return data_x, data_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3703d35",
   "metadata": {},
   "source": [
    "Let us study the distribution of lengths for proteins inside the folder to assess if padding is sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2d10840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at iteration: 3\r"
     ]
    }
   ],
   "source": [
    "#path_dir = \"C:/Users/Lucas/Desktop/Encoded_Proteins\"\n",
    "path_dir = \"./Encoded_Proteins_Toy\"\n",
    "lengths = np.zeros(len(os.listdir(path_dir)[1:5]))\n",
    "idx = 0\n",
    "for file in os.listdir(path_dir)[1:5]:\n",
    "    print(f\"We are at iteration: {idx}\", end=\"\\r\")\n",
    "    fpath = os.path.join(path_dir, file)\n",
    "    d = load_obj(fpath)\n",
    "    lengths[idx] = len(d['Native_Seq'])\n",
    "    idx+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e507a506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 1., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([ 57. ,  78.2,  99.4, 120.6, 141.8, 163. , 184.2, 205.4, 226.6,\n",
       "        247.8, 269. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAARYUlEQVR4nO3dXYxcZ33H8e+vJkYiRFDwEiK/YLfyBaEiIVoZUBAkF6ROALlIXNhCgBCRBUokqFokUyRA7Q0UlQsgYLlgBSpIbsDFEg4JqlBDQaHeIOfFBMPWpMrWETYEhVc1Nf33Yo7b6WZm59ie3XUefz/SaM55XmaeeXT2t2efPTOTqkKS1K4/WO0BSJKWl0EvSY0z6CWpcQa9JDXOoJekxj1rtQcwyrp162rz5s2rPQxJesa4//77f1ZVM6PqLsig37x5M3Nzc6s9DEl6xkjy7+PqXLqRpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjZsY9Ek2JvlWkkeSHE3y3hFtkuSTSeaTPJjkmqG67UmOdXV7pv0CJElL63NGfxr4i6p6KfAq4JYkVy5qcyOwtbvtBj4LkGQNcFtXfyWwa0RfSdIymhj0VfV4VX2/2/4V8AiwflGzHcAXa+A+4PlJrgC2AfNVdbyqngLu7NpKklbIWb0zNslm4BXA9xZVrQceG9pf6MpGlb9yzGPvZvDXAJs2bTqbYf0/m/d8/Zz7no9HP/qGVXleSZqk9z9jkzwX+Arwvqr65eLqEV1qifKnF1btq6rZqpqdmRn5cQ2SpHPQ64w+ySUMQv5LVfXVEU0WgI1D+xuAE8DaMeWSpBXS56qbAJ8HHqmqT4xpdhB4e3f1zauAJ6vqceAwsDXJliRrgZ1dW0nSCulzRn8t8DbgoSRHurK/AjYBVNVe4BBwEzAP/BZ4Z1d3OsmtwN3AGmB/VR2d5guQJC1tYtBX1b8weq19uE0Bt4ypO8TgF4EkaRX4zlhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMmfvFIkv3AG4GTVfUnI+rfD7x16PFeCsxU1RNJHgV+BfweOF1Vs9MauCSpnz5n9LcD28dVVtXHq+rqqroa+ADwz1X1xFCT67t6Q16SVsHEoK+qe4EnJrXr7ALuOK8RSZKmampr9Emew+DM/ytDxQXck+T+JLun9VySpP4mrtGfhTcB31m0bHNtVZ1I8iLgm0l+2P2F8DTdL4LdAJs2bZrisCTp4jbNq252smjZpqpOdPcngQPAtnGdq2pfVc1W1ezMzMwUhyVJF7epBH2S5wGvA742VHZpksvObAM3AA9P4/kkSf31ubzyDuA6YF2SBeDDwCUAVbW3a/Zm4J6q+s1Q18uBA0nOPM+Xq+ob0xu6JKmPiUFfVbt6tLmdwWWYw2XHgavOdWCSpOnwnbGS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuIlBn2R/kpNJRn7fa5LrkjyZ5Eh3+9BQ3fYkx5LMJ9kzzYFLkvrpc0Z/O7B9QptvV9XV3e2vAZKsAW4DbgSuBHYlufJ8BitJOnsTg76q7gWeOIfH3gbMV9XxqnoKuBPYcQ6PI0k6D9Nao391kgeS3JXkZV3ZeuCxoTYLXdlISXYnmUsyd+rUqSkNS5I0jaD/PvCSqroK+BTwj115RrStcQ9SVfuqaraqZmdmZqYwLEkSTCHoq+qXVfXrbvsQcEmSdQzO4DcONd0AnDjf55MknZ3zDvokL06Sbntb95g/Bw4DW5NsSbIW2AkcPN/nkySdnWdNapDkDuA6YF2SBeDDwCUAVbUXeAvwniSngd8BO6uqgNNJbgXuBtYA+6vq6LK8CknSWBODvqp2Taj/NPDpMXWHgEPnNjRJ0jT4zlhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklq3MSgT7I/yckkD4+pf2uSB7vbd5NcNVT3aJKHkhxJMjfNgUuS+ulzRn87sH2J+p8Ar6uqlwN/A+xbVH99VV1dVbPnNkRJ0vno852x9ybZvET9d4d27wM2TGFckqQpmfYa/buAu4b2C7gnyf1Jdi/VMcnuJHNJ5k6dOjXlYUnSxWviGX1fSa5nEPSvGSq+tqpOJHkR8M0kP6yqe0f1r6p9dMs+s7OzNa1xSdLFbipn9EleDnwO2FFVPz9TXlUnuvuTwAFg2zSeT5LU33kHfZJNwFeBt1XVj4bKL01y2Zlt4AZg5JU7kqTlM3HpJskdwHXAuiQLwIeBSwCqai/wIeCFwGeSAJzurrC5HDjQlT0L+HJVfWMZXoMkaQl9rrrZNaH+ZuDmEeXHgaue3kOStJJ8Z6wkNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1bmLQJ9mf5GSSkd/3moFPJplP8mCSa4bqtic51tXtmebAJUn99Dmjvx3YvkT9jcDW7rYb+CxAkjXAbV39lcCuJFeez2AlSWdvYtBX1b3AE0s02QF8sQbuA56f5ApgGzBfVcer6ingzq6tJGkFTfxy8B7WA48N7S90ZaPKXznuQZLsZvAXAZs2bZrCsFbW5j1fX7XnfvSjb1i155ZatFo/z8v1szyNf8ZmRFktUT5SVe2rqtmqmp2ZmZnCsCRJMJ0z+gVg49D+BuAEsHZMuSRpBU3jjP4g8Pbu6ptXAU9W1ePAYWBrki1J1gI7u7aSpBU08Yw+yR3AdcC6JAvAh4FLAKpqL3AIuAmYB34LvLOrO53kVuBuYA2wv6qOLsNrkCQtYWLQV9WuCfUF3DKm7hCDXwSSpFXiO2MlqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcb2CPsn2JMeSzCfZM6L+/UmOdLeHk/w+yQu6ukeTPNTVzU37BUiSltbnO2PXALcBrwcWgMNJDlbVD860qaqPAx/v2r8J+POqemLoYa6vqp9NdeSSpF76nNFvA+ar6nhVPQXcCexYov0u4I5pDE6SdP76BP164LGh/YWu7GmSPAfYDnxlqLiAe5Lcn2T3uCdJsjvJXJK5U6dO9RiWJKmPPkGfEWU1pu2bgO8sWra5tqquAW4Ebkny2lEdq2pfVc1W1ezMzEyPYUmS+ugT9AvAxqH9DcCJMW13smjZpqpOdPcngQMMloIkSSukT9AfBrYm2ZJkLYMwP7i4UZLnAa8DvjZUdmmSy85sAzcAD09j4JKkfiZedVNVp5PcCtwNrAH2V9XRJO/u6vd2Td8M3FNVvxnqfjlwIMmZ5/pyVX1jmi9AkrS0iUEPUFWHgEOLyvYu2r8duH1R2XHgqvMaoSTpvPjOWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWpcr6BPsj3JsSTzSfaMqL8uyZNJjnS3D/XtK0laXhO/SjDJGuA24PXAAnA4ycGq+sGipt+uqjeeY19J0jLpc0a/DZivquNV9RRwJ7Cj5+OfT19J0hT0Cfr1wGND+wtd2WKvTvJAkruSvOws+5Jkd5K5JHOnTp3qMSxJUh99gj4jymrR/veBl1TVVcCngH88i76Dwqp9VTVbVbMzMzM9hiVJ6qNP0C8AG4f2NwAnhhtU1S+r6tfd9iHgkiTr+vSVJC2vPkF/GNiaZEuStcBO4OBwgyQvTpJue1v3uD/v01eStLwmXnVTVaeT3ArcDawB9lfV0STv7ur3Am8B3pPkNPA7YGdVFTCy7zK9FknSCBODHv53OebQorK9Q9ufBj7dt68kaeX4zlhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqXK+gT7I9ybEk80n2jKh/a5IHu9t3k1w1VPdokoeSHEkyN83BS5Imm/hVgknWALcBrwcWgMNJDlbVD4aa/QR4XVX9IsmNwD7glUP111fVz6Y4bklST33O6LcB81V1vKqeAu4Edgw3qKrvVtUvut37gA3THaYk6Vz1Cfr1wGND+wtd2TjvAu4a2i/gniT3J9k9rlOS3UnmksydOnWqx7AkSX1MXLoBMqKsRjZMrmcQ9K8ZKr62qk4keRHwzSQ/rKp7n/aAVfsYLPkwOzs78vElSWevzxn9ArBxaH8DcGJxoyQvBz4H7Kiqn58pr6oT3f1J4ACDpSBJ0grpE/SHga1JtiRZC+wEDg43SLIJ+Crwtqr60VD5pUkuO7MN3AA8PK3BS5Imm7h0U1Wnk9wK3A2sAfZX1dEk7+7q9wIfAl4IfCYJwOmqmgUuBw50Zc8CvlxV31iWVyJJGqnPGj1VdQg4tKhs79D2zcDNI/odB65aXC5JWjm+M1aSGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa1yvok2xPcizJfJI9I+qT5JNd/YNJrunbV5K0vCYGfZI1wG3AjcCVwK4kVy5qdiOwtbvtBj57Fn0lScuozxn9NmC+qo5X1VPAncCORW12AF+sgfuA5ye5omdfSdIy6vPl4OuBx4b2F4BX9mizvmdfAJLsZvDXAMCvkxxb1GQd8LMe473o5GOA87MU52Zpzs/SVmx+up/lc/WScRV9gj4jyqpnmz59B4VV+4B9YweRzFXV7Lj6i53zM55zszTnZ2ktzE+foF8ANg7tbwBO9GyztkdfSdIy6rNGfxjYmmRLkrXATuDgojYHgbd3V9+8Cniyqh7v2VeStIwmntFX1ekktwJ3A2uA/VV1NMm7u/q9wCHgJmAe+C3wzqX6nuNYxy7rCHB+luLcLM35Wdozfn5SNXLJXJLUCN8ZK0mNM+glqXEXZNAneTTJQ0mOJJnryl6Q5JtJftzd/+Fqj3OlJNmf5GSSh4fKxs5Hkg90HzlxLMmfrs6oV86Y+flIkv/ojqEjSW4aqrto5ifJxiTfSvJIkqNJ3tuVe/yw5Py0dfxU1QV3Ax4F1i0q+1tgT7e9B/jYao9zBefjtcA1wMOT5oPBR008ADwb2AL8G7BmtV/DKszPR4C/HNH2opof4Argmm77MuBH3Rx4/Cw9P00dPxfkGf0YO4AvdNtfAP5s9YaysqrqXuCJRcXj5mMHcGdV/WdV/YTBlVDbVmKcq2XM/IxzUc1PVT1eVd/vtn8FPMLgHesePyw5P+M8I+fnQg36Au5Jcn/30QgAl9fg2ny6+xet2uguDOPmY9zHUVyMbu0+TXX/0NLERTs/STYDrwC+h8fP0yyaH2jo+LlQg/7aqrqGwade3pLktas9oGeQ3h870bjPAn8MXA08DvxdV35Rzk+S5wJfAd5XVb9cqumIsotxfpo6fi7IoK+qE939SeAAgz+Nftp9Iibd/cnVG+EFYdx89PnIiuZV1U+r6vdV9d/A3/N/f15fdPOT5BIGIfalqvpqV+zx0xk1P60dPxdc0Ce5NMllZ7aBG4CHGXx0wju6Zu8AvrY6I7xgjJuPg8DOJM9OsoXBdwT86yqMb1WdCbHOmxkcQ3CRzU+SAJ8HHqmqTwxVefwwfn6aO35W+7/BI/6r/UcM/qv9AHAU+GBX/kLgn4Afd/cvWO2xruCc3MHgz8f/YnBG8a6l5gP4IIOrAY4BN672+Fdpfv4BeAh4kMEP5xUX4/wAr2GwtPAgcKS73eTxM3F+mjp+/AgESWrcBbd0I0maLoNekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNe5/AG6QAF/oEbwqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9d65a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.5"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0561d766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d038c9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lengths>512)/31878 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0afc745a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lengths > 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf2a41",
   "metadata": {},
   "source": [
    "We have $4$ sequences over 1024, and $0.36\\%$ over 512. We can do what Christoph suggested of subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1daa6aec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'src_key_padding_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-31c83f6bfd09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m##We have to change how we save the output... necessarely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcouplings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#fields, couplings = decoder(data_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#seq_vals=torch.zeros(len(data_y), dtype=int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-6052383988ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, coupling_mask, padding_mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# (L x q x embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m## For the moment I suppose to have a single obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m## We have to output a matrix, not a vector, we hence do the opposite of CNNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'src_key_padding_mask'"
     ]
    }
   ],
   "source": [
    "n_cat:int = 21\n",
    "n_layers:int=2\n",
    "atten_dim:int=512\n",
    "embed_dim:int=5\n",
    "n_heads:int=16\n",
    "batch_size:int=4\n",
    "#dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=default_collate, shuffle=True)\n",
    "\n",
    "#decoder = Potts_Decoder(system_size, n_cat, n_layers, atten_dim, embed_dim, n_heads).to(device)\n",
    "lr = 0.001\n",
    "\n",
    "logging = []\n",
    "#device='cuda'\n",
    "device='cpu'\n",
    "#optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "decoder = Potts_Decoder(n_cat, n_layers, atten_dim, embed_dim, n_heads).to(device)\n",
    "\n",
    "\n",
    "data_x = d['Encoded_Protein'].reshape((1, 269, 512))\n",
    "data_y = d['Num_Seq']\n",
    "\n",
    "system_size=data_x.shape[1]\n",
    "mask: Tensor = torch.triu(torch.ones(system_size*decoder.n_cat, system_size*decoder.n_cat, dtype=bool), 1)  \n",
    "for i in range(system_size):\n",
    "    mask[i*n_cat : (i+1)*n_cat, i*n_cat : (i+1)*n_cat] = 0              \n",
    "mask: Tensor = torch.nn.Parameter(mask, requires_grad=False).to(device)\n",
    "##We have to change how we save the output... necessarely\n",
    "fields, couplings = decoder(data_x, mask)\n",
    "#fields, couplings = decoder(data_x)\n",
    "#seq_vals=torch.zeros(len(data_y), dtype=int)\n",
    "#for char,idx in zip(data_y, range(len(data_y))):\n",
    "#    seq_vals[idx] = letter_to_num[char]\n",
    "hot = one_hot(data_y, num_classes=decoder.n_cat)\n",
    "hot = torch.flatten(hot).unsqueeze(dim=0).to(device)\n",
    "#print(system_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b80a905b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'src_key_padding_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-a33d8628aab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matten_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'src_key_padding_mask'"
     ]
    }
   ],
   "source": [
    "layer = torch.nn.Sequential(*[TransformerEncoderLayer(atten_dim, n_heads, dropout=0.1, batch_first=True)]*2)\n",
    "layer.forward(data_x, src_key_padding_mask=None)\n",
    "\n",
    "## How to pass src_key_padding_mask when we have a nn.Sequential???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ad0d8dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (1): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77efdc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 269, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b799aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = energy_diffs(decoder, hot, fields, couplings, one_hot_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad9921dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -0.0000,  23.3927,  15.1612, -64.7930, -39.7287, -72.9019,  11.9301,\n",
       "         -9.1051, -12.5873,  27.2541,   4.0941, -62.8168, -20.7636,  -2.0990,\n",
       "        -37.6164,   7.0877,  62.4991, -65.0751, -23.8281, -26.2913,  29.7830],\n",
       "       grad_fn=<DiagBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6b39676",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = one_hot(data_y, num_classes=decoder.n_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "082adb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.argmax(prev[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0256388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42227f6d",
   "metadata": {
    "id": "2c7748d1-d62c-46c1-8477-815dcb70b0d1"
   },
   "outputs": [],
   "source": [
    "############ PATH FOR PERSONAL COMPUTER ################\n",
    "path_dir = \"./Encoded_Proteins_Toy\"\n",
    "\n",
    "########### PATH FOR OFFICE COMPUTER #################\n",
    "#path_dir = \"C:/Users/lucas/Desktop/Encoded_Proteins_Toy\"\n",
    "\n",
    "########## PATH FOR GOOGLE COLAB ####################\n",
    "#path_dir = \"./sample_data/Encoded_Proteins_Toy\"\n",
    "\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=4, collate_fn=default_collate, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e48ee199",
   "metadata": {
    "id": "7cc413a2-39e6-438d-9d86-ac043c6d5cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n"
     ]
    }
   ],
   "source": [
    "dataset = Encoded_Proteins(path_dir)\n",
    "print(len(dataset))\n",
    "dataset_train = Subset(dataset, np.arange(100))\n",
    "dataset_test = Subset(dataset, np.arange(start=100, stop=148))\n",
    "dataloader_train = DataLoader(dataset_train, collate_fn=default_collate, batch_size=4, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, collate_fn=default_collate, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "942553c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zt2R19QcwivD",
    "outputId": "bd079e12-74b4-46d2-cbbb-04ea37449a1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader_train) ### I have 250 batches of size 4 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b605204",
   "metadata": {
    "id": "TfdIAtrXEtts"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c77e9f",
   "metadata": {
    "id": "e6cf3cc2-f699-4698-891e-f985fce10c8b"
   },
   "outputs": [],
   "source": [
    "#dataset_train[1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d9b6a34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ca1157b-7d93-48b7-bd41-5c41266caaf6",
    "outputId": "bf7abfdd-73a0-44bf-d638-b22e6d556440",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at epoch:100, loss is:73.11790138483047\r"
     ]
    }
   ],
   "source": [
    "#system_size:int = data.shape[0]\n",
    "n_cat:int = 21\n",
    "n_layers:int=2\n",
    "atten_dim:int=512\n",
    "embed_dim:int=5\n",
    "n_heads:int=16\n",
    "batch_size:int=4\n",
    "#dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=default_collate, shuffle=True)\n",
    "\n",
    "#decoder = Potts_Decoder(system_size, n_cat, n_layers, atten_dim, embed_dim, n_heads).to(device)\n",
    "lr = 0.001\n",
    "\n",
    "logging = []\n",
    "#device='cuda'\n",
    "device='cpu'\n",
    "#optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "decoder = Potts_Decoder(n_cat, n_layers, atten_dim, embed_dim, n_heads).to(device)\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "loss_f = Pseudo_Likelihood\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "## Set the decoder to training mode\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    iterator=0\n",
    "    for data_xs, data_ys in dataloader_train:\n",
    "        print(f\"iterator:{iterator}\", end=\"\\r\")\n",
    "        iterator+=1\n",
    "        for data_x, data_y in zip(data_xs, data_ys):\n",
    "            data_x = data_x.unsqueeze(0).to(device)\n",
    "            system_size=data_x.shape[1]\n",
    "            mask: Tensor = torch.triu(torch.ones(system_size*decoder.n_cat, system_size*decoder.n_cat, dtype=bool), 1)  \n",
    "            for i in range(system_size):\n",
    "                mask[i*n_cat : (i+1)*n_cat, i*n_cat : (i+1)*n_cat] = 0              \n",
    "            mask: Tensor = torch.nn.Parameter(mask, requires_grad=False).to(device)\n",
    "            ##We have to change how we save the output... necessarely\n",
    "            fields, couplings = decoder(data_x, mask)\n",
    "            #fields, couplings = decoder(data_x)\n",
    "            #seq_vals=torch.zeros(len(data_y), dtype=int)\n",
    "            #for char,idx in zip(data_y, range(len(data_y))):\n",
    "            #    seq_vals[idx] = letter_to_num[char]\n",
    "            hot = one_hot(data_y, num_classes=decoder.n_cat)\n",
    "            hot = torch.flatten(hot).unsqueeze(dim=0).to(device)\n",
    "            #print(system_size)\n",
    "\n",
    "            loss = loss_f(decoder, hot, fields, couplings, one_hot_input=True)/batch_size\n",
    "            total_loss += float(loss)\n",
    "            loss.backward()\n",
    "        optimizer.step()\n",
    "    losses_train.append(total_loss)\n",
    "    print(f\"We are at epoch:{epoch}, loss is:{total_loss}\", end=\"\\r\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    #################################### TESTING #########################################\n",
    "    total_loss_test = 0\n",
    "    #losses_test = []\n",
    "    decoder.eval()\n",
    "    for data_xs, data_ys in dataloader_test:\n",
    "        for data_x, data_y in zip(data_xs, data_ys):\n",
    "            data_x = data_x.unsqueeze(0).to(device)\n",
    "            system_size=data_x.shape[1]\n",
    "            mask: Tensor = torch.triu(torch.ones(system_size*decoder.n_cat, system_size*decoder.n_cat, dtype=bool), 1)  \n",
    "            for i in range(system_size):\n",
    "                mask[i*n_cat : (i+1)*n_cat, i*n_cat : (i+1)*n_cat] = 0              \n",
    "            mask: Tensor = torch.nn.Parameter(mask, requires_grad=False).to(device)\n",
    "            ##We have to change how we save the output... necessarely\n",
    "            fields, couplings = decoder(data_x, mask)\n",
    "            #fields, couplings = decoder(data_x)\n",
    "            #seq_vals=torch.zeros(len(data_y), dtype=int)\n",
    "            #for char,idx in zip(data_y, range(len(data_y))):\n",
    "            #    seq_vals[idx] = letter_to_num[char]\n",
    "            hot = one_hot(data_y, num_classes=decoder.n_cat)\n",
    "            hot = torch.flatten(hot).unsqueeze(dim=0).to(device)\n",
    "            #print(system_size)\n",
    "\n",
    "            loss = loss_f(decoder, hot, fields, couplings, one_hot_input=True)/batch_size\n",
    "            total_loss_test += float(loss)\n",
    "    \n",
    "    losses_test.append(total_loss_test)\n",
    "    #train_acc = test(train_loader)\n",
    "    #test_acc = test(test_loader)\n",
    "    #logging.append({\"Epoch\": epoch, \"value\": total_loss, \"kind\": \"Loss\"})\n",
    "    #logging.append({\"Epoch\": epoch, \"value\": train_acc, \"kind\": \"Train_acc\"})\n",
    "    #logging.append({\"Epoch\": epoch, \"value\": test_acc, \"kind\": \"Test\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453902ce",
   "metadata": {
    "id": "cd88501e-375d-417e-96e3-03fde6f109f0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a69c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Pq6x6R5y2yKw",
    "outputId": "7fcdd452-96df-4126-cf81-7c180a1edccc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2e141e4a50>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe555be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idrTLft92z4Y",
    "outputId": "4a102e9b-0ee7-4af9-ca87-27f4dd136935"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34.974768579006195]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d29590",
   "metadata": {
    "id": "c2Qugpho225-"
   },
   "source": [
    "## LET US TRY TO BUILD A NEW LOADER\n",
    "- This loader has to be able to deal with batches which first patches to a common dimension and then returns them patched.\n",
    "- We have to make sure patched parts are ignored from the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0bcc551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `tensor.detach()` not found.\n"
     ]
    }
   ],
   "source": [
    "?tensor.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7eac8905",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = \"./Encoded_Proteins_Toy\"\n",
    "fname = os.listdir(path_dir)[0:2]\n",
    "fpath1 = os.path.join(path_dir, fname[0])\n",
    "d1 = load_obj(fpath1)\n",
    "\n",
    "fpath2 = os.path.join(path_dir, fname[1])\n",
    "d2 = load_obj(fpath2)\n",
    "## This just auxualiry\n",
    "d_list = [d1, d2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f880540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is supposed to batch to a common dimension. You also have to batch the output!!!!\n",
    "def parse_batch(batch):\n",
    "    ## Here batch has to be an iterable of proteins structures\n",
    "    B = len(batch)\n",
    "    ## Maybe not maximum?\n",
    "    L_max = max([len(b['Num_Seq']) for b in batch])\n",
    "    #X = np.zeros([B, L_max, 4, 3], dtype=np.float32)\n",
    "    #S = np.zeros([B, L_max], dtype=np.int32)\n",
    "    X = torch.zeros([B, L_max, 512], dtype=torch.float32)\n",
    "    S = torch.zeros([B, L_max], dtype=torch.int8)\n",
    "    \n",
    "    for i, b in enumerate(batch):\n",
    "        l = len(b['Native_Seq'])\n",
    "        x = b['Encoded_Protein'].detach().numpy()\n",
    "\n",
    "        X[i] = torch.tensor(np.pad(x, [[0, L_max-l], [0,0]], 'constant', constant_values=(np.nan, )))\n",
    "        S[i, :l] = b['Num_Seq'] ## Do we leave to 0 all the other values?\n",
    "            \n",
    "    isnan = torch.isnan(X)\n",
    "    mask = torch.isfinite(torch.sum(X, axis=2)) #.astype(torch.bool)\n",
    "    X[isnan] = 0.\n",
    "    X = torch.nan_to_num(X)\n",
    "    \n",
    "    return X, S, mask\n",
    "\n",
    "## This function is supposed to batch to a common dimension. You also have to batch the output!!!!\n",
    "class DynamicLoader(): \n",
    "    def __init__(self, dataset, batch_size=3000, shuffle=True): \n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def batch(self):\n",
    "        dataset = self.dataset\n",
    "        lengths = [len(b['seq']) for b in dataset]\n",
    "\n",
    "        clusters, batch = [], []\n",
    "        for ix in np.argsort(lengths):\n",
    "            size = lengths[ix]\n",
    "            if size * (len(batch) + 1) <= self.batch_size:\n",
    "                batch.append(ix)\n",
    "            else:\n",
    "                if len(batch) > 0: clusters.append(batch)\n",
    "                batch = [ix]\n",
    "        if len(batch) > 0:\n",
    "            clusters.append(batch)\n",
    "        self.clusters = clusters\n",
    "        print(len(clusters), 'batches', len(dataset), 'structures')\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.batch()\n",
    "        if self.shuffle: np.random.shuffle(self.clusters)\n",
    "        N = len(self.clusters)\n",
    "        for b_idx in self.clusters[:N]:\n",
    "            batch = [self.dataset[i] for i in b_idx]\n",
    "            yield parse_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5c34842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5256bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005121946334838867\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X, S, mask = parse_batch(d_list)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d430404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc6c5d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d2['Native_Seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568584a",
   "metadata": {},
   "source": [
    "Code for Homologous sequences by Christoph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e118e73",
   "metadata": {},
   "source": [
    "#/bin/bash\n",
    "\n",
    "\n",
    "cathfile=$1\n",
    "maxcon=100\n",
    "\n",
    "for id in $(awk -F'|' '/>/{split($3, a, \"/\"); pdb=substr(a[1], 1, 4); chain=substr(a[1], 5, 1); printf \"%s_%s\\n\",pdb,chain}' cath-dataset-nonredundant-S20.fa | head -n 1000); do\n",
    "\taws s3 cp --no-sign-request s3://openfold/pdb/${id}/a3m/uniref90_hits.a3m ./${id}.a3m &\n",
    "\tn=$(jobs -p | wc -l)\n",
    "    \tif [[ $n -ge $maxcon ]]; then\n",
    "        \techo \"waiting for jobs to finish ($n running)\"\n",
    "        \tsleep 1\n",
    "\tfi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22e00172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Pass the input through the encoder layer.\n",
       "\n",
       "Args:\n",
       "    src: the sequence to the encoder layer (required).\n",
       "    src_mask: the mask for the src sequence (optional).\n",
       "    src_key_padding_mask: the mask for the src keys per batch (optional).\n",
       "\n",
       "Shape:\n",
       "    see the docs in Transformer class.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/transformer.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?TransformerEncoderLayer.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4697a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "New_Decoder.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
