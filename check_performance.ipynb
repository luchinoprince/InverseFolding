{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to assess if the dynamic batching improves the training speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/miniconda3/envs/InvFolding/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from encoded_protein_dataset import EncodedProteinDataset, collate_fn, get_embedding, dynamic_collate_fn\n",
    "from pseudolikelihood import get_npll\n",
    "import torch\n",
    "import numpy as np\n",
    "from potts_decoder import PottsDecoder\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "#import pandas as pd\n",
    "import csv\n",
    "from torch.autograd import profiler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(decoder, inputs, eta, device):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    couplings, fields = decoder(encodings, padding_mask)\n",
    "\n",
    "    # embed and reshape to (B, M, N*q)\n",
    "    msas_embedded = embedding(msas).view(B, M, -1)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll(msas_embedded, couplings, fields, N, q)\n",
    "\n",
    "\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    penalty = eta*(torch.sum(couplings**2) + torch.sum(fields**2))/B\n",
    "\n",
    "    # the padding mask does not contain the msa dimension so we need to multiply by M\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    loss_penalty = npll_mean  + penalty\n",
    "\n",
    "    return loss_penalty, npll_mean.item()  ##we can just add the penalty since we have set already to 0 couplings and fields of padded elements\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_loader_dyn(decoder, loader, eta, device):\n",
    "    decoder.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for effective_batch_size, inputs_packed in loader:\n",
    "            npll_full = 0\n",
    "            for inputs in inputs_packed:\n",
    "                mini_batch_size = inputs[0].shape[0]\n",
    "                _, npll = get_loss(decoder, inputs, eta, device) \n",
    "                npll_full += npll*mini_batch_size/effective_batch_size\n",
    "            losses.append(npll_full)\n",
    "            #del inputs\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def train_dyn(decoder, inputs_packed, eta, device):\n",
    "    effective_batch_size = inputs_packed[0]\n",
    "    loss_penalty_full = 0\n",
    "    train_loss_full = 0\n",
    "    for inputs in inputs_packed[1]:\n",
    "        mini_batch_size = inputs[0].shape[0]\n",
    "        loss_penalty, train_batch_loss = get_loss(decoder, inputs, eta, device)    ## get the current loss for the batch\n",
    "        loss_penalty = loss_penalty * mini_batch_size/effective_batch_size\n",
    "        train_batch_loss = train_batch_loss * mini_batch_size/effective_batch_size\n",
    "        loss_penalty.backward()                         ## Get gradients\n",
    "        loss_penalty_full += loss_penalty.detach()\n",
    "        train_loss_full += train_batch_loss\n",
    "    \n",
    "    optimizer.step()   \n",
    "    optimizer.zero_grad()                           ## set previous gradients to 0\n",
    "\n",
    "    return loss_penalty_full, train_loss_full\n",
    "\n",
    "def get_loss(decoder, inputs, eta, device):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    couplings, fields = decoder(encodings, padding_mask)\n",
    "\n",
    "    # embed and reshape to (B, M, N*q)\n",
    "    msas_embedded = embedding(msas).view(B, M, -1)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll(msas_embedded, couplings, fields, N, q)\n",
    "\n",
    "\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    penalty = eta*(torch.sum(couplings**2) + torch.sum(fields**2))/B\n",
    "\n",
    "    # the padding mask does not contain the msa dimension so we need to multiply by M\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    loss_penalty = npll_mean + penalty\n",
    "\n",
    "    return loss_penalty, npll_mean.item()  ##we can just add the penalty since we have set already to 0 couplings and fields of padded elements\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_loader(decoder, loader, eta, device):\n",
    "\n",
    "    decoder.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            _, npll = get_loss(decoder, inputs, eta, device) \n",
    "            losses.append(npll)\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is:3, Counter fail 1:0, Counter fail 2:0, length data:3\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silval/InverseFolding/InverseFolding_git/encoded_protein_dataset.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter is:1002, Counter fail 1:0, Counter fail 2:0, length data:999\r"
     ]
    }
   ],
   "source": [
    "## The max_msas optional parameter in the EncodedProteinDataset library allows to select just a subset of the folders of that size \n",
    "### Takes roughly 35 minutes... sometimes 4... makes no sense...\n",
    "##lenght of training data should be 22559\n",
    "train_dataset = EncodedProteinDataset('/Data/InverseFoldingData/msas/train', '/Data/InverseFoldingData/structure_encodings', max_msas=1000, noise=0.02)\n",
    "sequence_test_dataset = EncodedProteinDataset('/Data/InverseFoldingData/msas/test/sequence', '/Data/InverseFoldingData/structure_encodings', max_msas=1000)\n",
    "structure_test_dataset = EncodedProteinDataset('/Data/InverseFoldingData/msas/test/structure', '/Data/InverseFoldingData/structure_encodings', max_msas=1000)\n",
    "superfamily_test_dataset = EncodedProteinDataset('/Data/InverseFoldingData/msas/test/superfamily', '/Data/InverseFoldingData/structure_encodings', max_msas=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_structure_size = 4   ### I think with empty GPU we can go up to 10\n",
    "perc_subset_test = 0.1     ## During the training, for every dataset available we select a random 10% of its samples\n",
    "batch_msa_size = 16\n",
    "q = 21 ##isn't always 21\n",
    "\n",
    "## Static Loader\n",
    "collate_fn_stat = partial(collate_fn, q=q, batch_msa_size=batch_msa_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_structure_size, collate_fn=collate_fn_stat, shuffle=True)\n",
    "\n",
    "sequence_test_loader = DataLoader(sequence_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn_stat, shuffle=False, \n",
    "sampler=RandomSampler(sequence_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(sequence_test_dataset))))\n",
    "\n",
    "structure_test_loader = DataLoader(structure_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn_stat, shuffle=False, \n",
    "sampler=RandomSampler(structure_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(structure_test_dataset))))\n",
    "\n",
    "superfamily_test_loader = DataLoader(superfamily_test_dataset, batch_size=batch_structure_size, collate_fn=collate_fn_stat, shuffle=False, \n",
    "sampler=RandomSampler(superfamily_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(superfamily_test_dataset))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dynamic loader\n",
    "q=21\n",
    "perc_subset_test = 0.1     ## During the training, for every dataset available we select a random 10% of its samples\n",
    "batch_msa_size = 16\n",
    "batch_structure_size_dyn = 20   ### I think with empty GPU we can go up to 10\n",
    "collate_fn_dyn = partial(dynamic_collate_fn, q=q, batch_size=batch_structure_size_dyn, batch_msa_size=batch_msa_size)\n",
    "\n",
    "train_loader_dyn = DataLoader(train_dataset, batch_size=batch_structure_size_dyn, collate_fn=collate_fn_dyn, shuffle=True)\n",
    "\n",
    "sequence_test_loader_dyn = DataLoader(sequence_test_dataset, batch_size=batch_structure_size_dyn, collate_fn=collate_fn_dyn, shuffle=False, \n",
    "sampler=RandomSampler(sequence_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(sequence_test_dataset))))\n",
    "\n",
    "structure_test_loader_dyn = DataLoader(structure_test_dataset, batch_size=batch_structure_size_dyn, collate_fn=collate_fn_dyn, shuffle=False, \n",
    "sampler=RandomSampler(structure_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(structure_test_dataset))))\n",
    "\n",
    "superfamily_test_loader_dyn = DataLoader(superfamily_test_dataset, batch_size=batch_structure_size_dyn, collate_fn=collate_fn_dyn, shuffle=False, \n",
    "sampler=RandomSampler(superfamily_test_dataset, replacement=True, num_samples=int(perc_subset_test*len(superfamily_test_dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_step: 199, epoch: 3.98 train_batch: 71.30 train: 67.57, sequence: 61.91, structure: 73.49, superfamily: 62.27, max_gpu:0: 100%|█████████▉| 199/200 [04:54<00:01,  1.48s/it]       \n"
     ]
    }
   ],
   "source": [
    "decoder = None\n",
    "embedding = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed = 0\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "update_steps = 200                 ##Usual values are update steps=10^5, test_steps=10^2\n",
    "test_steps = 20\n",
    "n_epochs = update_steps//(len(train_dataset)//batch_structure_size_dyn)   ## the other update steps will be used for \"partial epochs\", I want to save the last complet epoch\n",
    "\n",
    "input_encoding_dim = train_dataset.encoding_dim\n",
    "param_embed_dim = 512\n",
    "n_param_heads = 4\n",
    "d_model = 128\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "## Check before running which is the GPU which is free the most and put it as the running device\n",
    "device = 3\n",
    "eta = 1e-3\n",
    "dropout = 0.0\n",
    "#print(check_gpu_mem())\n",
    "\n",
    "\n",
    "decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "decoder.to(device)\n",
    "embedding = get_embedding(q)\n",
    "embedding.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "with tqdm(total = update_steps) as pbar: ##This is used to have the nice loading bar while training\n",
    "    train_loss = 0\n",
    "    max_gpu = 0\n",
    "    update_step = 0\n",
    "    bk_iter = int(1e4)                             ## This tells us how ofter we save a model(default values is every ten-thousand updates)\n",
    "    bk_dir = \"./../IntermediateModels/\"       ## Folder to where we save the intermediate models\n",
    "    train_batch_losses = []\n",
    "    epoch = 0.0\n",
    "    while update_step < update_steps:\n",
    "        for inputs_packed in train_loader_dyn:\n",
    "            ##This packs a batch in a good way for memory reasons\n",
    "            update_step += 1                                ## Increase update step (the update steps will count also different batches within the same epoch)\n",
    "            epoch = update_step / len(train_loader_dyn)\n",
    "            #for inputs in inputs_packed:\n",
    "            loss_penalty, train_batch_loss = train_dyn(decoder, inputs_packed, eta, device)\n",
    "\n",
    "\n",
    "            train_batch_losses.append(train_batch_loss) ## Here we append the lossess in the different batches within the same epoch\n",
    "            \n",
    "            ## We want to keep track of the test loss not at every batch, too costrly otherwise. Usually set to once every 100.\n",
    "            if (update_step==1 or update_step % test_steps == 0) or update_step == update_steps:\n",
    "                train_loss = np.mean(train_batch_losses)\n",
    "                del loss_penalty\n",
    "                del train_batch_losses\n",
    "                \n",
    "                ## Lossess for the different test sets, want to use a subset of this only. Also want to pass only a random subset of it if possible\n",
    "                structure_test_loss = get_loss_loader_dyn(decoder, structure_test_loader_dyn, eta, device)\n",
    "                sequence_test_loss = get_loss_loader_dyn(decoder, sequence_test_loader_dyn, eta, device)\n",
    "                superfamily_test_loss = get_loss_loader_dyn(decoder, superfamily_test_loader_dyn, eta, device)\n",
    "\n",
    "                \n",
    "                train_batch_losses = []\n",
    "            if update_step >= update_steps:\n",
    "                break\n",
    "            pbar.set_description(f'update_step: {update_step}, epoch: {epoch:.2f} train_batch: {train_batch_loss:.2f} train: {train_loss:.2f}, sequence: {sequence_test_loss:.2f}, structure: {structure_test_loss:.2f}, superfamily: {superfamily_test_loss:.2f}, max_gpu:{max_gpu}')#, GPU total memory: {check_gpu_mem().values[device, 0]}, GPU used: {check_gpu_mem().values[device, 1]}')\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_step: 1000, epoch: 4.00 train_batch: 15.34 train: 10.42, sequence: 9.73, structure: 10.94, superfamily: 8.68, max_gpu:0: 100%|██████████| 1000/1000 [08:41<00:00,  1.92it/s]        \n"
     ]
    }
   ],
   "source": [
    "decoder = None\n",
    "embedding = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seed = 0\n",
    "torch.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "update_steps = 5*200                 ##Usual values are update steps=10^5, test_steps=10^2\n",
    "test_steps = 5*20\n",
    "n_epochs = update_steps//(len(train_dataset)//batch_structure_size_dyn)   ## the other update steps will be used for \"partial epochs\", I want to save the last complet epoch\n",
    "\n",
    "input_encoding_dim = train_dataset.encoding_dim\n",
    "param_embed_dim = 512\n",
    "n_param_heads = 4\n",
    "d_model = 128\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "## Check before running which is the GPU which is free the most and put it as the running device\n",
    "device = 3\n",
    "eta = 1e-3\n",
    "dropout = 0.0\n",
    "#print(check_gpu_mem())\n",
    "\n",
    "\n",
    "decoder = PottsDecoder(q, n_layers, d_model, input_encoding_dim, param_embed_dim, n_heads, n_param_heads, dropout=dropout)\n",
    "decoder.to(device)\n",
    "embedding = get_embedding(q)\n",
    "embedding.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "with tqdm(total = update_steps) as pbar: ##This is used to have the nice loading bar while training\n",
    "    train_loss = 0\n",
    "    max_gpu = 0\n",
    "    update_step = 0\n",
    "    bk_iter = int(1e4)                             ## This tells us how ofter we save a model(default values is every ten-thousand updates)\n",
    "    bk_dir = \"./../IntermediateModels/\"       ## Folder to where we save the intermediate models\n",
    "    train_batch_losses = []\n",
    "    epoch = 0.0\n",
    "    while update_step < update_steps:\n",
    "        for inputs in train_loader:\n",
    "            ##This packs a batch in a good way for memory reasons\n",
    "            update_step += 1                                ## Increase update step (the update steps will count also different batches within the same epoch)\n",
    "            epoch = update_step / len(train_loader)\n",
    "            #for inputs in inputs_packed:\n",
    "            loss_penalty, train_batch_loss = get_loss(decoder, inputs, eta, device)    ## get the current loss for the batch\n",
    "            optimizer.zero_grad()                           ## set previous gradients to 0\n",
    "            loss_penalty.backward()    \n",
    "            loss_penalty.detach()\n",
    "            optimizer.step()     \n",
    "\n",
    "            train_batch_losses.append(train_batch_loss) ## Here we append the lossess in the different batches within the same epoch\n",
    "            \n",
    "            ## We want to keep track of the test loss not at every batch, too costrly otherwise. Usually set to once every 100.\n",
    "            if (update_step==1 or update_step % test_steps == 0) or update_step == update_steps:\n",
    "                train_loss = np.mean(train_batch_losses)\n",
    "                del loss_penalty\n",
    "                del train_batch_losses\n",
    "                \n",
    "                ## Lossess for the different test sets, want to use a subset of this only. Also want to pass only a random subset of it if possible\n",
    "                structure_test_loss = get_loss_loader(decoder, structure_test_loader, eta, device)\n",
    "                sequence_test_loss = get_loss_loader(decoder, sequence_test_loader, eta, device)\n",
    "                superfamily_test_loss = get_loss_loader(decoder, superfamily_test_loader, eta, device)\n",
    "\n",
    "                \n",
    "                train_batch_losses = []\n",
    "            if update_step>update_steps:\n",
    "                break\n",
    "            pbar.set_description(f'update_step: {update_step}, epoch: {epoch:.2f} train_batch: {train_batch_loss:.2f} train: {train_loss:.2f}, sequence: {sequence_test_loss:.2f}, structure: {structure_test_loss:.2f}, superfamily: {superfamily_test_loss:.2f}, max_gpu:{max_gpu}')#, GPU total memory: {check_gpu_mem().values[device, 0]}, GPU used: {check_gpu_mem().values[device, 1]}')\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('InvFolding')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03d8b451b8fdc532a54f23be046d4bcc8dafe2ef49a4f04a10424f9541af615f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
