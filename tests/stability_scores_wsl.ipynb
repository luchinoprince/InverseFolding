{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e5a335",
   "metadata": {},
   "source": [
    "In this notebook I am going to try to create the template for evaluating the stability scores of the predicted structures of the low temperatures samples from both ESM and Potts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db82a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyrosetta\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import biotite.structure\n",
    "from biotite.structure.io import pdbx, pdb\n",
    "from biotite.structure.residues import get_residues\n",
    "from biotite.structure import filter_backbone\n",
    "from biotite.structure import get_chains\n",
    "from biotite.sequence import ProteinSequence\n",
    "from typing import Sequence, Tuple, List\n",
    "from Bio import SeqIO\n",
    "\n",
    "git_folder = '/mnt/c/Users/Luca/OneDrive/Phd/Second_year/research/Feinauer/InverseFolding'\n",
    "esm_folder = '/mnt/c/Users/Luca/OneDrive/Phd/Second_year/research/Feinauer/esm/'\n",
    "sys.path.insert(1, os.path.join(git_folder, 'model'))\n",
    "sys.path.insert(1, os.path.join(git_folder, 'util'))\n",
    "\n",
    "sys.path.insert(1, esm_folder)\n",
    "from potts_decoder import PottsDecoder\n",
    "from ioutils import read_fasta, read_encodings\n",
    "import esm\n",
    "#from esm.inverse_folding import util\n",
    "import esm.pretrained as pretrained\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea6d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_structure(fpath, chain=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fpath: filepath to either pdb or cif file\n",
    "        chain: the chain id or list of chain ids to load\n",
    "    Returns:\n",
    "        biotite.structure.AtomArray\n",
    "    \"\"\"\n",
    "    with open(fpath) as fin:\n",
    "        pdbf = pdb.PDBFile.read(fin)\n",
    "    structure = pdb.get_structure(pdbf, model=1)\n",
    "    bbmask = filter_backbone(structure)\n",
    "    structure = structure[bbmask]\n",
    "    all_chains = get_chains(structure)\n",
    "    if len(all_chains) == 0:\n",
    "        raise ValueError('No chains found in the input file.')\n",
    "    if chain is None:\n",
    "        chain_ids = all_chains\n",
    "    elif isinstance(chain, list):\n",
    "        chain_ids = chain\n",
    "    else:\n",
    "        chain_ids = [chain] \n",
    "    for chain in chain_ids:\n",
    "        if chain not in all_chains:\n",
    "            raise ValueError(f'Chain {chain} not found in input file')\n",
    "    chain_filter = [a.chain_id in chain_ids for a in structure]\n",
    "    structure = structure[chain_filter]\n",
    "    return structure\n",
    "\n",
    "def extract_coords_from_structure(structure: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        structure: An instance of biotite AtomArray\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates\n",
    "            - seq is the extracted sequence\n",
    "    \"\"\"\n",
    "    coords = get_atom_coords_residuewise([\"N\", \"CA\", \"C\"], structure)\n",
    "    residue_identities = get_residues(structure)[1]\n",
    "    seq = ''.join([ProteinSequence.convert_letter_3to1(r) for r in residue_identities])\n",
    "    return coords, seq\n",
    "\n",
    "def get_atom_coords_residuewise(atoms: List[str], struct: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Example for atoms argument: [\"N\", \"CA\", \"C\"]\n",
    "    \"\"\"\n",
    "    def filterfn(s, axis=None):\n",
    "        filters = np.stack([s.atom_name == name for name in atoms], axis=1)\n",
    "        sum = filters.sum(0)\n",
    "        if not np.all(sum <= np.ones(filters.shape[1])):\n",
    "            raise RuntimeError(\"structure has multiple atoms with same name\")\n",
    "        index = filters.argmax(0)\n",
    "        coords = s[index].coord\n",
    "        coords[sum == 0] = float(\"nan\")\n",
    "        return coords\n",
    "\n",
    "    return biotite.structure.apply_residue_wise(struct, struct, filterfn)\n",
    "\n",
    "def get_loss_new(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    param_embeddings, fields = decoder.forward_new(encodings, padding_mask)\n",
    "    msas_embedded = embedding(msas)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll2(msas_embedded, param_embeddings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    \n",
    "    Q = torch.einsum('bkuia, buhia->bkhia', \n",
    "                param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1)\n",
    "    penalty = eta*(torch.sum(torch.sum(Q,axis=-1)**2) - torch.sum(Q**2) + torch.sum(fields**2))/B\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "\n",
    "def get_loss(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    couplings, fields = decoder(encodings, padding_mask)\n",
    "\n",
    "    # embed and reshape to (B, M, N*q)\n",
    "    msas_embedded = embedding(msas).view(B, M, -1)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll(msas_embedded, couplings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    penalty = eta*(torch.sum(couplings**2) + torch.sum(fields**2))/B\n",
    "\n",
    "    # the padding mask does not contain the msa dimension so we need to multiply by M\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "    #return loss_penalty\n",
    "\n",
    "def get_loss_loader(decoder, loader, eta):\n",
    "\n",
    "    decoder.eval()\n",
    "    losses = 0\n",
    "    iterator = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            iterator+=1\n",
    "            _, npll = get_loss_new(decoder, inputs, eta) \n",
    "            losses+=npll\n",
    "    \n",
    "    return losses/iterator\n",
    "\n",
    "def compute_covariance(msa, q):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix of a given MSA having q different amino acids\n",
    "    \"\"\"\n",
    "    M, N = msa.shape\n",
    "\n",
    "    # One hot encode classes and reshape to create data matrix\n",
    "    D = torch.flatten(one_hot(msa, num_classes=q), start_dim=1).to(torch.float32)\n",
    "\n",
    "    # Remove one amino acid\n",
    "    D = D.view(M, N, q)[:, :, :q-1].flatten(1)\n",
    "\n",
    "    # Compute bivariate frequencies\n",
    "    bivariate_freqs = D.T @ D / M\n",
    "    \n",
    "    # Compute product of univariate frequencies\n",
    "    univariate_freqs = torch.diagonal(bivariate_freqs).view(N*(q-1), 1) @ torch.diagonal(bivariate_freqs).view(1, N*(q-1))\n",
    "\n",
    "    return bivariate_freqs - univariate_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f3782",
   "metadata": {},
   "source": [
    "## Generate low-temperature samples from Potts Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d5acdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing sampler... 0.0110323 sec\n",
      "\n",
      "sampling model with mcmc... 1.15917 sec\n",
      "updating mcmc stats with samples... 0.00599692 sec\n",
      "computing sequence energies and correlations... 0.0126774 sec\n",
      "decreasing wait time to 600\n",
      "writing final sequences... done\n"
     ]
    }
   ],
   "source": [
    "auxiliary_model_dir = '/mnt/d/Data/InverseFolding/Auxiliary_Potts'\n",
    "out_dir = '/mnt/d/Data/InverseFolding/Auxiliary_Samples_Potts'\n",
    "out_file = 'samples_lowT.txt'\n",
    "samples_path = os.path.join(auxiliary_model_dir, \"YAP1_HUMAN_couplings_fields.txt\")\n",
    "## The ! creates a terminal command, to pass variable you need to put square brackets\n",
    "!(bmdca_sample -p {samples_path} -d {out_dir} -o {out_file} -n 100 -c bmdca_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ac20702",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet='ACDEFGHIKLMNPQRSTVWY-'\n",
    "default_index = alphabet.index('-')\n",
    "aa_index = defaultdict(lambda: default_index, {alphabet[i]: i for i in range(len(alphabet))})\n",
    "aa_index_inv = dict(map(reversed, aa_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f89dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file='samples_lowT_numerical.txt'\n",
    "with open(os.path.join(out_dir,file), mode='r') as f:\n",
    "    lines=f.readlines()\n",
    "\n",
    "char_seq = [] ##36 is the lenght of YAP\n",
    "\n",
    "for i in range(1, len(lines)):\n",
    "    line = lines[i][0:-1].split(\" \") ## I take out the end of file\n",
    "    line_char = [aa_index_inv[int(idx)] for idx in line]\n",
    "    char_seq.append(line_char)\n",
    "    \n",
    "## Now re-translate\n",
    "for prot_idx in range(len(char_seq)):\n",
    "    for aa in range(len(char_seq[prot_idx])):\n",
    "        char_seq[prot_idx][aa] = aa_index[char_seq[prot_idx][aa]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd400624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12, 12, 20, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       17, 12, 12, 12, 12, 12, 12, 14, 12, 12, 20,  8, 12, 12,  5, 12, 14,\n",
       "       12, 20, 20, 20, 12, 12, 12, 12, 12, 12, 12, 18, 12, 12, 20, 12, 12,\n",
       "       20, 12,  9, 12, 12, 12, 12, 12, 20,  3, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12,  0, 20,  4,  0, 10, 20, 12, 12, 12, 12, 12, 12, 12,  7, 20,\n",
       "       12,  7,  4, 12, 12,  1, 20, 12, 20,  1, 12, 20, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 20, 12, 12, 12, 12, 20, 12,  5, 13, 20, 12,\n",
       "       12, 12, 12,  3, 12, 20, 12, 10, 12, 12, 12, 12,  0, 12, 17, 16, 17,\n",
       "        6, 19, 12, 12,  3, 12, 18, 20, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 20, 12, 12, 12, 12, 20, 12, 12, 12,  0, 12, 12, 12, 12, 20, 12,\n",
       "       12, 20, 12, 12, 12, 12, 20, 10, 12, 12, 20,  2, 12, 12, 12, 19, 18,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12,  6, 12, 12, 12, 12, 20, 20, 12, 12,\n",
       "       12, 12, 12,  8, 12, 17, 15, 18, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12,  6, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 20, 12, 12,\n",
       "       12, 13,  4, 12, 12, 15, 12, 12, 12, 12, 18, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 20, 20, 20, 18,  1,  0, 12, 12,  6, 12, 12, 12,  0, 12,\n",
       "       12, 12, 12, 12, 20, 12, 12, 12,  2, 17, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12,  6, 12, 20, 12, 12, 12, 12, 12, 12, 12, 20, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 20, 12, 12, 12, 17, 19,  9, 12, 12, 12,\n",
       "        5, 12, 12,  5, 12, 12, 18, 12, 12, 12, 12, 18, 12, 12, 12, 12, 12,\n",
       "       12, 20, 12, 12,  4, 12, 12, 12, 12, 12,  4, 18, 12, 12, 12, 12, 12,\n",
       "       12, 12, 20, 12,  7, 12, 20, 20, 12, 12, 12, 12, 12,  5, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12,  7,  2, 20, 18,  8, 12, 12,  2, 12, 12, 12,  6,  0,\n",
       "       12, 12, 12, 12, 12, 12, 12,  5, 12, 12, 12,  2, 12, 12, 12, 12, 12,\n",
       "       12, 20, 12,  2, 12, 11, 12, 12, 12, 12, 12, 12, 15, 12, 12, 12,  0,\n",
       "       17, 12, 12, 12, 12, 12, 12, 12, 20, 12,  4, 12,  0, 12, 12, 12,  5,\n",
       "       12, 12, 13, 12, 12, 12, 12, 12, 20, 12, 12, 12, 12, 12, 20, 12, 20,\n",
       "       20, 10, 12,  5, 12, 12, 19, 12, 12, 12, 12, 12, 12, 12,  2, 12, 20,\n",
       "       12, 12, 12, 12, 12, 12, 12,  2, 16, 12, 12,  0, 12, 20, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12,  2,  7, 12,  6,  8, 12, 12, 12, 12, 12, 12,\n",
       "       12,  9, 12, 12,  2, 12, 12, 12, 12, 12, 12, 12, 20, 20, 20, 12, 20,\n",
       "        8, 12, 20,  0, 12, 12, 20, 20, 16,  5, 12,  6, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 20, 12, 12, 12, 12,  7, 20, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 12, 12, 12,  7, 12, 12, 12,\n",
       "       12, 20, 12, 12, 12, 12, 20, 12, 12, 12, 12, 12,  5, 20, 12, 12,  4,\n",
       "       12, 17, 12, 12, 20, 12, 12, 12, 12, 12, 12, 20, 12, 12, 12, 13, 20,\n",
       "       12, 12, 11, 17, 12, 12, 12, 12, 20, 12, 12, 12, 20, 20, 12, 12, 12,\n",
       "       12, 12, 12, 20, 12, 12, 17, 12, 12, 12, 20, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 20, 12, 12, 12, 19, 12,  0,  1, 12, 20, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12,  2, 12, 12, 12, 12, 20, 12, 12, 12, 20, 12,  3,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 20, 20,  9, 12,\n",
       "       12, 12, 12, 12, 20, 12, 12, 20, 12, 12, 19, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 20, 12, 12, 14, 12, 12, 12, 12,  6, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 20, 12, 12, 12,  4,  9, 12, 12, 12, 12, 20,\n",
       "       12, 12, 12, 12, 12, 12,  2, 12, 12,  4, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 16, 12, 12, 12, 12, 13, 20,  1, 13, 12, 12,  5,\n",
       "        0, 12, 12, 12, 20, 12, 12, 12, 12, 20, 12, 12, 12, 15, 12, 17, 12,\n",
       "       12, 12,  5, 16, 12, 12, 18, 12,  3,  5, 12, 12, 17, 12, 12, 12, 12,\n",
       "        5, 12, 12, 12, 12, 12, 20, 12, 12, 12, 20, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 20, 20, 12, 12, 12, 12, 12, 12, 20, 12,\n",
       "       20,  0, 12,  0, 19, 12, 12, 16, 12, 12, 12, 12, 12, 10, 12, 12, 12,\n",
       "       12,  0, 20, 12, 12, 12, 12, 12, 12, 12, 12, 20, 12, 12, 12,  9, 14,\n",
       "       11, 12, 12, 12, 12, 12, 12, 12, 20, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "       12, 12, 20, 12,  2, 12, 12,  9, 12, 12, 12, 20, 12, 20, 12, 12,  7,\n",
       "       12, 12, 12,  7, 12, 12, 12, 15, 12, 12, 12, 12, 12, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(char_seq)[::,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "82a573b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, 20, 12, 12,  5, 18,  3, 14, 17, 16, 16,  2,  2,  5, 14,\n",
       "        9, 19, 19, 19,  2,  6, 14, 16,  5, 16, 16, 16, 18, 19,  2, 12, 20,\n",
       "       20, 20])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(char_seq)[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4258d2",
   "metadata": {},
   "source": [
    "...... still have to insert AF2 part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814acda",
   "metadata": {},
   "source": [
    "## ESM sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9cfeea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutational_dir = '/mnt/d/Data/InverseFolding/Mutational_Data'\n",
    "msas_folder = '/mnt/d/Data/InverseFolding/Mutational_Data/alphafold_results_wildtype'\n",
    "\n",
    "protein_original_DMS = 'YAP1_HUMAN_1_b0.5.a2m.wildtype.fasta'\n",
    "structure_name = 'YAP1_HUMAN_1_b0.5.a2m_unrelaxed_rank_1_model_5.pdb'\n",
    "\n",
    "folder_fasta = os.path.join(mutational_dir, 'alignments')\n",
    "native_path = os.path.join(folder_fasta, protein_original_DMS)\n",
    "structure_folder = os.path.join(mutational_dir, 'alphafold_results_wildtype')\n",
    "structure_path = os.path.join(structure_folder, structure_name)\n",
    "\n",
    "num_seq = read_fasta(native_path, mutated_exp=True)\n",
    "structure =  load_structure(structure_path)\n",
    "coords, native_seq = extract_coords_from_structure(structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d749740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Luca/OneDrive/Phd/Second_year/research/Feinauer/esm/esm/pretrained.py:174: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model, alphabet = pretrained.esm_if1_gvp4_t16_142M_UR50() \n",
    "model.eval();\n",
    "rep = esm.inverse_folding.util.get_encoder_output(model, alphabet, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ab078bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at sample 99 out of 10000\r"
     ]
    }
   ],
   "source": [
    "samples_esm = []\n",
    "for attempt in range(100):\n",
    "    print(f\"We are at sample {attempt} out of {10000}\", end=\"\\r\")\n",
    "    sample = model.sample(coords, temperature=0.1)\n",
    "    seq_num = []\n",
    "    for char in sample:\n",
    "        seq_num.append(aa_index[char])\n",
    "    samples_esm.append(seq_num)\n",
    "    #samples_esm.append(model.sample(coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "54dd6167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 3, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(samples_esm)[:,35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d87ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
