{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I want to compare the pseudo-likelihood I obtain using ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/miniconda3/envs/InvFolding/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from encoded_protein_dataset import EncodedProteinDataset, collate_fn, get_embedding\n",
    "#from pseudolikelihood import get_npll\n",
    "import torch\n",
    "from potts_decoder import PottsDecoder\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from functools import partial\n",
    "import biotite.structure\n",
    "from biotite.structure.io import pdbx, pdb\n",
    "from biotite.structure.residues import get_residues\n",
    "from biotite.structure import filter_backbone\n",
    "from biotite.structure import get_chains\n",
    "from biotite.sequence import ProteinSequence\n",
    "from typing import Sequence, Tuple, List\n",
    "import scipy\n",
    "from Bio import SeqIO\n",
    "\n",
    "import os\n",
    "##TURIN HPC\n",
    "sys.path.insert(1, \"/Data/silva/esm/\")\n",
    "\n",
    "## EUROPA\n",
    "#sys.path.insert(1, \"/home/lucasilva/esm/\")\n",
    "import esm\n",
    "#from esm.inverse_folding import util\n",
    "import esm.pretrained as pretrained\n",
    "from ioutils import read_fasta, read_encodings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity calculation:\n",
    "\n",
    "- We would like to compare our results with the perplexity of the paper of ESM, unfortunately we cannot calculate the likelihood of our predicted model, as that is the central problem of Potts models.\n",
    "- We would like to assess if we could calculate the pseudo likelihood for the model given by ESM and compare it with our results to get a benchmark to measure against.\n",
    "- We shall remember that our task is harder, as we don't give the model the exact native sequence, but give a batch of the MSA in which we often don't have the true sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now we have everything, we can hence write a code that gives us the the pseudolikelihood of esm to get a benchmark for our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main observation is that:\n",
    "$$ p(y_i|y_{-i}) = \\frac{p(y)}{p(y_{-i})} = \\frac{p(y)}{\\sum_{a}p(y_i=a, y_{-i})}, $$\n",
    "which, since it involves marginalizing over just one position of the sequence is clearly computationally feasible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprev_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoder_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mincremental_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeatures_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Args:\n",
      "    prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
      "        `(batch, tgt_len)`, for teacher forcing\n",
      "    encoder_out (optional): output from the encoder, used for\n",
      "        encoder-side attention, should be of size T x B x C\n",
      "    incremental_state (dict): dictionary used for storing state during\n",
      "        :ref:`Incremental decoding`\n",
      "    features_only (bool, optional): only return features without\n",
      "        applying output layer (default: False).\n",
      "\n",
      "Returns:\n",
      "    tuple:\n",
      "        - the decoder's output of shape `(batch, tgt_len, vocab)`\n",
      "        - a dictionary with any model-specific outputs\n",
      "\u001b[0;31mFile:\u001b[0m      /Data/silva/esm/esm/inverse_folding/transformer_decoder.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "?decoder.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_loss(model, alphabet, coords, seq, device=None):\n",
    "    batch_converter = CoordBatchConverter(alphabet)\n",
    "    batch = [(coords, None, seq)]\n",
    "    coords, confidence, strs, tokens, padding_mask = batch_converter(batch, device=device)\n",
    "    \n",
    "    prev_output_tokens = tokens[:, :-1]\n",
    "    target = tokens[:, 1:]\n",
    "    target_padding_mask = (target == alphabet.padding_idx)\n",
    "    logits, _ = model.forward(coords, padding_mask, confidence, prev_output_tokens)\n",
    "    loss = F.cross_entropy(logits, target, reduction='none')\n",
    "    #loss = loss[0].detach().numpy()\n",
    "    #loss = loss[0].to('cpu').numpy()\n",
    "    #target_padding_mask = target_padding_mask[0].numpy()\n",
    "    return loss, target_padding_mask\n",
    "\n",
    "def get_sequence_loss_decoder(decoder, alphabet, coords, encodings, seq, device=None):\n",
    "    batch_converter = CoordBatchConverter(alphabet)\n",
    "    batch = [(coords, None, seq)]\n",
    "    coords, confidence, strs, tokens, padding_mask = batch_converter(batch, device=device)\n",
    "    \n",
    "    prev_output_tokens = tokens[:, :-1]\n",
    "    target = tokens[:, 1:]\n",
    "    target_padding_mask = (target == alphabet.padding_idx)\n",
    "    #encoder_padding_mask = torch.zeros(1, N, dtype=bool).to(device)  ## This will be needed if we batch\n",
    "    encoder_output = {'encoder_out':[encodings], 'encoder_padding_mask':[]}\n",
    "    logits, _ = decoder.forward(prev_output_tokens, encoder_output)#encoder_output)\n",
    "    loss = F.cross_entropy(logits, target, reduction='none')\n",
    "    return loss, target_padding_mask\n",
    "\n",
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, truncation_seq_length: int = None):\n",
    "        self.alphabet = alphabet\n",
    "        self.truncation_seq_length = truncation_seq_length\n",
    "\n",
    "    def __call__(self, raw_batch: Sequence[Tuple[str, str]]):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        batch_labels, seq_str_list = zip(*raw_batch)\n",
    "        seq_encoded_list = [self.alphabet.encode(seq_str) for seq_str in seq_str_list]\n",
    "        if self.truncation_seq_length:\n",
    "            seq_encoded_list = [seq_str[:self.truncation_seq_length] for seq_str in seq_encoded_list]\n",
    "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
    "        tokens = torch.empty(\n",
    "            (\n",
    "                batch_size,\n",
    "                max_len + int(self.alphabet.prepend_bos) + int(self.alphabet.append_eos),\n",
    "            ),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        tokens.fill_(self.alphabet.padding_idx)\n",
    "        labels = []\n",
    "        strs = []\n",
    "\n",
    "        for i, (label, seq_str, seq_encoded) in enumerate(\n",
    "            zip(batch_labels, seq_str_list, seq_encoded_list)\n",
    "        ):\n",
    "            labels.append(label)\n",
    "            strs.append(seq_str)\n",
    "            if self.alphabet.prepend_bos:\n",
    "                tokens[i, 0] = self.alphabet.cls_idx\n",
    "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
    "            tokens[\n",
    "                i,\n",
    "                int(self.alphabet.prepend_bos) : len(seq_encoded)\n",
    "                + int(self.alphabet.prepend_bos),\n",
    "            ] = seq\n",
    "            if self.alphabet.append_eos:\n",
    "                tokens[i, len(seq_encoded) + int(self.alphabet.prepend_bos)] = self.alphabet.eos_idx\n",
    "\n",
    "        return labels, strs, tokens\n",
    "\n",
    "\n",
    "class CoordBatchConverter(BatchConverter):\n",
    "    def __call__(self, raw_batch: Sequence[Tuple[Sequence, str]], device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_batch: List of tuples (coords, confidence, seq)\n",
    "            In each tuple,\n",
    "                coords: list of floats, shape L x 3 x 3\n",
    "                confidence: list of floats, shape L; or scalar float; or None\n",
    "                seq: string of length L\n",
    "        Returns:\n",
    "            coords: Tensor of shape batch_size x L x 3 x 3\n",
    "            confidence: Tensor of shape batch_size x L\n",
    "            strs: list of strings\n",
    "            tokens: LongTensor of shape batch_size x L\n",
    "            padding_mask: ByteTensor of shape batch_size x L\n",
    "        \"\"\"\n",
    "        self.alphabet.cls_idx = self.alphabet.get_idx(\"<cath>\") \n",
    "        batch = []\n",
    "        for coords, confidence, seq in raw_batch:\n",
    "            if confidence is None:\n",
    "                confidence = 1.\n",
    "            if isinstance(confidence, float) or isinstance(confidence, int):\n",
    "                confidence = [float(confidence)] * len(coords)\n",
    "            if seq is None:\n",
    "                seq = 'X' * len(coords)\n",
    "            batch.append(((coords, confidence), seq))\n",
    "\n",
    "        coords_and_confidence, strs, tokens = super().__call__(batch)\n",
    "\n",
    "        # pad beginning and end of each protein due to legacy reasons\n",
    "        coords = [\n",
    "            F.pad(torch.tensor(cd), (0, 0, 0, 0, 1, 1), value=np.inf)\n",
    "            for cd, _ in coords_and_confidence\n",
    "        ]\n",
    "        confidence = [\n",
    "            F.pad(torch.tensor(cf), (1, 1), value=-1.)\n",
    "            for _, cf in coords_and_confidence\n",
    "        ]\n",
    "        coords = self.collate_dense_tensors(coords, pad_v=np.nan)\n",
    "        confidence = self.collate_dense_tensors(confidence, pad_v=-1.)\n",
    "        if device is not None:\n",
    "            coords = coords.to(device)\n",
    "            confidence = confidence.to(device)\n",
    "            tokens = tokens.to(device)\n",
    "        padding_mask = torch.isnan(coords[:,:,0,0])\n",
    "        coord_mask = torch.isfinite(coords.sum(-2).sum(-1))\n",
    "        confidence = confidence * coord_mask + (-1.) * padding_mask\n",
    "        return coords, confidence, strs, tokens, padding_mask\n",
    "\n",
    "    def from_lists(self, coords_list, confidence_list=None, seq_list=None, device=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coords_list: list of length batch_size, each item is a list of\n",
    "            floats in shape L x 3 x 3 to describe a backbone\n",
    "            confidence_list: one of\n",
    "                - None, default to highest confidence\n",
    "                - list of length batch_size, each item is a scalar\n",
    "                - list of length batch_size, each item is a list of floats of\n",
    "                    length L to describe the confidence scores for the backbone\n",
    "                    with values between 0. and 1.\n",
    "            seq_list: either None or a list of strings\n",
    "        Returns:\n",
    "            coords: Tensor of shape batch_size x L x 3 x 3\n",
    "            confidence: Tensor of shape batch_size x L\n",
    "            strs: list of strings\n",
    "            tokens: LongTensor of shape batch_size x L\n",
    "            padding_mask: ByteTensor of shape batch_size x L\n",
    "        \"\"\"\n",
    "        batch_size = len(coords_list)\n",
    "        if confidence_list is None:\n",
    "            confidence_list = [None] * batch_size\n",
    "        if seq_list is None:\n",
    "            seq_list = [None] * batch_size\n",
    "        raw_batch = zip(coords_list, confidence_list, seq_list)\n",
    "        return self.__call__(raw_batch, device)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_dense_tensors(samples, pad_v):\n",
    "        \"\"\"\n",
    "        Takes a list of tensors with the following dimensions:\n",
    "            [(d_11,       ...,           d_1K),\n",
    "             (d_21,       ...,           d_2K),\n",
    "             ...,\n",
    "             (d_N1,       ...,           d_NK)]\n",
    "        and stack + pads them into a single tensor of:\n",
    "        (N, max_i=1,N { d_i1 }, ..., max_i=1,N {diK})\n",
    "        \"\"\"\n",
    "        if len(samples) == 0:\n",
    "            return torch.Tensor()\n",
    "        if len(set(x.dim() for x in samples)) != 1:\n",
    "            raise RuntimeError(\n",
    "                f\"Samples has varying dimensions: {[x.dim() for x in samples]}\"\n",
    "            )\n",
    "        (device,) = tuple(set(x.device for x in samples))  # assumes all on same device\n",
    "        max_shape = [max(lst) for lst in zip(*[x.shape for x in samples])]\n",
    "        result = torch.empty(\n",
    "            len(samples), *max_shape, dtype=samples[0].dtype, device=device\n",
    "        )\n",
    "        result.fill_(pad_v)\n",
    "        for i in range(len(samples)):\n",
    "            result_i = result[i]\n",
    "            t = samples[i]\n",
    "            result_i[tuple(slice(0, k) for k in t.shape)] = t\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_npll(msas_embedded, couplings, fields, N, q):\n",
    "    \"\"\" Get negative pseudo log likelihood (npll)\n",
    "    \"\"\"\n",
    "\n",
    "    B, M, _ = msas_embedded.shape\n",
    "\n",
    "    # (B, M, N*q) x (B, N*q, N*q) + (B, 1, N*q) -> (B, M, N*q) -> (B, M, N, q)\n",
    "    A = (msas_embedded @ couplings + fields.unsqueeze(1)).view(B, M, N, q)\n",
    "\n",
    "    # (B, M, N, q) -> (B, M, N)\n",
    "    Z = torch.logsumexp(A, dim=-1)\n",
    "\n",
    "    # (B, M, N, q) * (B, M, N, q) -> (B, M, N, q) -> (B, M, N)\n",
    "    C = torch.sum(A * msas_embedded.view(B, M, N, q), dim=-1)\n",
    "\n",
    "    # (B, M, N) - (B, M, N) -> (B, M, N)\n",
    "    pll = C - Z\n",
    "\n",
    "    return -pll\n",
    "def load_structure(fpath, chain=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fpath: filepath to either pdb or cif file\n",
    "        chain: the chain id or list of chain ids to load\n",
    "    Returns:\n",
    "        biotite.structure.AtomArray\n",
    "    \"\"\"\n",
    "    with open(fpath) as fin:\n",
    "        pdbf = pdb.PDBFile.read(fin)\n",
    "    structure = pdb.get_structure(pdbf, model=1)\n",
    "    bbmask = filter_backbone(structure)\n",
    "    structure = structure[bbmask]\n",
    "    all_chains = get_chains(structure)\n",
    "    if len(all_chains) == 0:\n",
    "        raise ValueError('No chains found in the input file.')\n",
    "    if chain is None:\n",
    "        chain_ids = all_chains\n",
    "    elif isinstance(chain, list):\n",
    "        chain_ids = chain\n",
    "    else:\n",
    "        chain_ids = [chain] \n",
    "    for chain in chain_ids:\n",
    "        if chain not in all_chains:\n",
    "            raise ValueError(f'Chain {chain} not found in input file')\n",
    "    chain_filter = [a.chain_id in chain_ids for a in structure]\n",
    "    structure = structure[chain_filter]\n",
    "    return structure\n",
    "\n",
    "def extract_coords_from_structure(structure: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        structure: An instance of biotite AtomArray\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates\n",
    "            - seq is the extracted sequence\n",
    "    \"\"\"\n",
    "    coords = get_atom_coords_residuewise([\"N\", \"CA\", \"C\"], structure)\n",
    "    residue_identities = get_residues(structure)[1]\n",
    "    seq = ''.join([ProteinSequence.convert_letter_3to1(r) for r in residue_identities])\n",
    "    return coords, seq\n",
    "\n",
    "def get_atom_coords_residuewise(atoms: List[str], struct: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Example for atoms argument: [\"N\", \"CA\", \"C\"]\n",
    "    \"\"\"\n",
    "    def filterfn(s, axis=None):\n",
    "        filters = np.stack([s.atom_name == name for name in atoms], axis=1)\n",
    "        sum = filters.sum(0)\n",
    "        if not np.all(sum <= np.ones(filters.shape[1])):\n",
    "            raise RuntimeError(\"structure has multiple atoms with same name\")\n",
    "        index = filters.argmax(0)\n",
    "        coords = s[index].coord\n",
    "        coords[sum == 0] = float(\"nan\")\n",
    "        return coords\n",
    "\n",
    "    return biotite.structure.apply_residue_wise(struct, struct, filterfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalize(model, coords, native_seq, restr_alphabet, alphabet, i, device=None):\n",
    "    \"\"\" This gives the normalizing factor at different positions, old version which uses the whole model\"\"\"\n",
    "    if native_seq[i] not in restr_alphabet:\n",
    "        raise ValueError(\"Error! Character is not in the resitricted dataset.\")\n",
    "    ## I want to calculate the probability for the restricted dataset \n",
    "    norm = 0\n",
    "    N = len(native_seq)\n",
    "    for char in restr_alphabet:\n",
    "        mutated_seq = list(native_seq)\n",
    "        mutated_seq[i] = char\n",
    "        mutated_seq = \"\".join(mutated_seq)\n",
    "        #res = esm.inverse_folding.util.score_sequence(model, alphabet_esm, coords, mutated_seq)[0]\n",
    "        res=get_sequence_loss(model, alphabet, coords, mutated_seq, device=device)[0]#.item()\n",
    "        #norm += np.exp(-torch.sum(res).item()/N)\n",
    "        norm += np.exp(-torch.sum(res).item())\n",
    "    return norm\n",
    "\n",
    "def get_loss_esm(model, restr_alphabet, alphabet_esm, coords, native_seq, device=None):\n",
    "    \"\"\"Old version using the whole model at every pass\"\"\"\n",
    "    N = coords.shape[0]\n",
    "    part = 0\n",
    "    for i in range(N):\n",
    "        norm = marginalize(model, coords, native_seq, restr_alphabet, alphabet_esm, i, device=device)\n",
    "        res=get_sequence_loss(model, alphabet_esm, coords, native_seq, device=device)[0]\n",
    "        #part += np.exp(-torch.sum(res)[0].item()/N)/norm\n",
    "        part += (-torch.sum(res).item() - np.log(norm))\n",
    "    return part/N\n",
    "\n",
    "\n",
    "def marginalize_decoder(decoder, coords, encodings, native_seq, restr_alphabet, alphabet, i, device=None):\n",
    "    \"\"\" This gives the normalizing factor at different positions, new version using just the decoder\"\"\"\n",
    "    if native_seq[i] not in restr_alphabet:\n",
    "        raise ValueError(\"Error! Character is not in the resitricted dataset.\")\n",
    "    ## I want to calculate the probability for the restricted dataset \n",
    "    norm = 0\n",
    "    N = len(native_seq)\n",
    "    for char in restr_alphabet:\n",
    "        mutated_seq = list(native_seq)\n",
    "        mutated_seq[i] = char\n",
    "        mutated_seq = \"\".join(mutated_seq)\n",
    "        #res = esm.inverse_folding.util.score_sequence(model, alphabet_esm, coords, mutated_seq)[0]\n",
    "        res=get_sequence_loss_decoder(decoder, alphabet, coords, encodings, mutated_seq, device=device)[0]#.item()\n",
    "        #norm += np.exp(-torch.sum(res).item()/N)\n",
    "        norm += np.exp(-torch.sum(res).item())\n",
    "    return norm\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_esm_decoder(decoder, restr_alphabet, alphabet_esm, coords, encodings, native_seq, device=None):\n",
    "    \"\"\"New version using just the decoder\"\"\"\n",
    "    N = len(native_seq)\n",
    "    part = 0\n",
    "    for i in range(N):\n",
    "        norm = marginalize_decoder(decoder, coords, encodings, native_seq, restr_alphabet, alphabet_esm, i, device=device)\n",
    "        res=get_sequence_loss_decoder(decoder, alphabet_esm, coords, encodings, native_seq, device=device)[0]\n",
    "        #part += np.exp(-torch.sum(res)[0].item()/N)/norm\n",
    "        part += (-torch.sum(res).item() - np.log(norm))\n",
    "    return part/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/esm/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at iteration 0 out of 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/miniconda3/envs/InvFolding/lib/python3.9/site-packages/biotite/structure/io/pdb/file.py:419: UserWarning: 3012 elements were guessed from atom_name.\n",
      "  warn(\"{} elements were guessed from atom_name.\".format(rep_num))\n"
     ]
    }
   ],
   "source": [
    "model, alphabet = pretrained.esm_if1_gvp4_t16_142M_UR50() \n",
    "model.eval();\n",
    "\n",
    "device=0\n",
    "model.to(device)\n",
    "\n",
    "ab = 'ACDEFGHIKLMNPQRSTVWY-'\n",
    "\n",
    "\n",
    "structure_dir = '/Data/christoph/bocconi/dompdb'\n",
    "encodings_folder = '/Data/InverseFoldingData/structure_encodings'\n",
    "checks=1\n",
    "check=0\n",
    "npll = np.zeros(checks)\n",
    "\n",
    "#for pdb_name in (os.listdir(structure_dir)):\n",
    "for encoding_files in (os.listdir(encodings_folder)):\n",
    "    pdb_name = encoding_files[0:7]\n",
    "    print(f\"We are at iteration {check} out of {checks}\", end=\"\\r\")\n",
    "    aux = {}\n",
    "    pdb_path = os.path.join(structure_dir, pdb_name)\n",
    "    structure =  load_structure(pdb_path)\n",
    "    coords, native_seq = extract_coords_from_structure(structure)\n",
    "    #coords = torch.from_numpy(coords)\n",
    "    #lppds[check] = np.log(marginalize(model, coords, native_seq, ab, alphabet, 1, device=device))\n",
    "    npll[check] = get_loss_esm(model, ab, alphabet, coords, native_seq, device=device)\n",
    "    check+=1\n",
    "    if check>=checks:\n",
    "        break\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(native_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00190027])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/esm/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at iteration 0 out of 1\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/miniconda3/envs/InvFolding/lib/python3.9/site-packages/biotite/structure/io/pdb/file.py:419: UserWarning: 3012 elements were guessed from atom_name.\n",
      "  warn(\"{} elements were guessed from atom_name.\".format(rep_num))\n",
      "/tmp/ipykernel_3388142/2759275659.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n",
      "/tmp/ipykernel_3388142/2759275659.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encodings = torch.tensor(encodings).unsqueeze(1).to(device)\n"
     ]
    }
   ],
   "source": [
    "model, alphabet = pretrained.esm_if1_gvp4_t16_142M_UR50() \n",
    "decoder = model.decoder\n",
    "decoder.eval();\n",
    "\n",
    "device=0\n",
    "decoder.to(device)\n",
    "\n",
    "ab = 'ACDEFGHIKLMNPQRSTVWY-'\n",
    "\n",
    "structure_dir = '/Data/christoph/bocconi/dompdb'\n",
    "encodings_folder = '/Data/InverseFoldingData/structure_encodings'\n",
    "checks=1\n",
    "check=0\n",
    "npll2 = np.zeros(checks)\n",
    "\n",
    "for encoding_files in (os.listdir(encodings_folder)):\n",
    "    id = encoding_files[0:7]\n",
    "    encoding_path = os.path.join(encodings_folder, encoding_files)\n",
    "    print(f\"We are at iteration {check} out of {checks}\", end=\"\\r\")\n",
    "    aux = {}\n",
    "    pdb_path = os.path.join(structure_dir, id)\n",
    "    structure =  load_structure(pdb_path)\n",
    "    coords, native_seq = extract_coords_from_structure(structure)\n",
    "    encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n",
    "    encodings = torch.tensor(encodings).unsqueeze(1).to(device)\n",
    "    \n",
    "    npll2[check] = get_loss_esm_decoder(decoder, ab, alphabet, coords, encodings, native_seq, device=device)\n",
    "    check+=1\n",
    "    break\n",
    "    #batch_converter = CoordBatchConverter(alphabet)\n",
    "    #batch = [(coords, None, native_seq)]\n",
    "    #coords, confidence, strs, tokens, padding_mask = batch_converter(batch, device=device)\n",
    "    \n",
    "    #encodings = torch.tensor(read_encodings(encoding_path, trim=False)).to(device)\n",
    "    #prev_output_tokens = tokens[:, :-1]\n",
    "    #target = tokens[:, 1:]\n",
    "    #encodings = torch.tensor(encodings).unsqueeze(1)\n",
    "    #encoder_padding_mask = torch.zeros(1, N, dtype=bool).to(device)\n",
    "    #encoder_output = {'encoder_out':[encodings], 'encoder_padding_mask':[encoder_padding_mask]}\n",
    "    #encoder_padding_mask = torch.zeros(1, N, dtype=bool)\n",
    "    #logits, _ = decoder.forward(prev_output_tokens, encodings)#encoder_output)\n",
    "    #loss = F.cross_entropy(logits, target, reduction='none')\n",
    "    #coords = torch.from_numpy(coords)\n",
    "    #lppds[check] = np.log(marginalize(model, coords, native_seq, ab, alphabet, 1, device=device))\n",
    "    #npll[check] = get_loss_esm(model, ab, alphabet, coords, native_seq, device=device)\n",
    "    #check+=1\n",
    "    #if check>=checks:\n",
    "    #    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/silva/miniconda3/envs/InvFolding/lib/python3.9/site-packages/biotite/structure/io/pdb/file.py:419: UserWarning: 3012 elements were guessed from atom_name.\n",
      "  warn(\"{} elements were guessed from atom_name.\".format(rep_num))\n"
     ]
    }
   ],
   "source": [
    "structure =  load_structure(pdb_path)\n",
    "coords, native_seq = extract_coords_from_structure(structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 3, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(native_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.encoder\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "decoder.eval()\n",
    "structure =  load_structure(pdb_path)\n",
    "coords, native_seq = extract_coords_from_structure(structure)\n",
    "\n",
    "#encodings = torch.tensor(read_encodings(encoding_path, trim=False))\n",
    "#encodings = torch.tensor(encodings).unsqueeze(1).to(device)\n",
    "batch_converter = CoordBatchConverter(alphabet)\n",
    "batch = [(coords, None, native_seq)]\n",
    "coords, confidence, strs, tokens, padding_mask = batch_converter(batch, device=device)\n",
    "\n",
    "encoder_output = encoder.forward(coords, padding_mask, confidence)\n",
    "\n",
    "prev_output_tokens = tokens[:, :-1]\n",
    "target = tokens[:, 1:]\n",
    "target_padding_mask = (target == alphabet.padding_idx)\n",
    "#encoder_padding_mask = torch.zeros(1, N, dtype=bool).to(device)  ## This will be needed if we batch\n",
    "#encoder_output = {'encoder_out':[encodings], 'encoder_padding_mask':[]}\n",
    "logits, _ = decoder.forward(prev_output_tokens, encoder_output)#encoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_output['encoder_out'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)\n",
    "model.eval()\n",
    "structure =  load_structure(pdb_path)\n",
    "coords, native_seq = extract_coords_from_structure(structure)\n",
    "batch_converter = CoordBatchConverter(alphabet)\n",
    "batch = [(coords, None, native_seq)]\n",
    "coords, confidence, strs, tokens, padding_mask = batch_converter(batch, device=device)\n",
    "\n",
    "prev_output_tokens = tokens[:, :-1]\n",
    "target = tokens[:, 1:]\n",
    "target_padding_mask = (target == alphabet.padding_idx)\n",
    "logits2, _ = model.forward(coords, padding_mask, confidence, prev_output_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBLEM\n",
    "-   The forward method of ESM does add a beginnng/end of sequence, so to make comparison we have to do a forward pass because for our task we do not add this dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([378, 1, 512])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-21.7387, -21.7923, -14.6354, -21.7338,   0.4261], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, 0:5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-21.7387, -21.7923, -14.6354, -21.7338,   0.4261], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2[0, 0:5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 35, 378])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 35, 378])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits, target, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 378])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([378, 1, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output['encoder_out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([378, 1, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.60031303])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprev_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoder_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mincremental_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeatures_only\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Args:\n",
      "    prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
      "        `(batch, tgt_len)`, for teacher forcing\n",
      "    encoder_out (optional): output from the encoder, used for\n",
      "        encoder-side attention, should be of size T x B x C\n",
      "    incremental_state (dict): dictionary used for storing state during\n",
      "        :ref:`Incremental decoding`\n",
      "    features_only (bool, optional): only return features without\n",
      "        applying output layer (default: False).\n",
      "\n",
      "Returns:\n",
      "    tuple:\n",
      "        - the decoder's output of shape `(batch, tgt_len, vocab)`\n",
      "        - a dictionary with any model-specific outputs\n",
      "\u001b[0;31mFile:\u001b[0m      /Data/silva/esm/esm/inverse_folding/transformer_decoder.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "decoder = model.decoder\n",
    "?decoder.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mencoder_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mconfidence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Args:\n",
      "    coords (Tensor): backbone coordinates\n",
      "        shape batch_size x num_residues x num_atoms (3 for N, CA, C) x 3\n",
      "    encoder_padding_mask (ByteTensor): the positions of\n",
      "          padding elements of shape `(batch_size x num_residues)`\n",
      "    confidence (Tensor): the confidence score of shape (batch_size x\n",
      "        num_residues). The value is between 0. and 1. for each residue\n",
      "        coordinate, or -1. if no coordinate is given\n",
      "    return_all_hiddens (bool, optional): also return all of the\n",
      "        intermediate hidden states (default: False).\n",
      "\n",
      "Returns:\n",
      "    dict:\n",
      "        - **encoder_out** (Tensor): the last encoder layer's output of\n",
      "          shape `(num_residues, batch_size, embed_dim)`\n",
      "        - **encoder_padding_mask** (ByteTensor): the positions of\n",
      "          padding elements of shape `(batch_size, num_residues)`\n",
      "        - **encoder_embedding** (Tensor): the (scaled) embedding lookup\n",
      "          of shape `(batch_size, num_residues, embed_dim)`\n",
      "        - **encoder_states** (List[Tensor]): all intermediate\n",
      "          hidden states of shape `(num_residues, batch_size, embed_dim)`.\n",
      "          Only populated if *return_all_hiddens* is True.\n",
      "\u001b[0;31mFile:\u001b[0m      /Data/silva/esm/esm/inverse_folding/gvp_transformer_encoder.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "encoder = model.encoder\n",
    "?encoder.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 14 2023, 21:23:14)  [GCC 12.2.0 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
