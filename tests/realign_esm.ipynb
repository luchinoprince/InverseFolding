{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook we will try to use the **pyhmmer** package to realign the sequences generated by esm to the one of the msa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import biotite.structure\n",
    "from biotite.structure.io import pdbx, pdb\n",
    "from biotite.structure.residues import get_residues\n",
    "from biotite.structure import filter_backbone\n",
    "from biotite.structure import get_chains\n",
    "from biotite.sequence import ProteinSequence\n",
    "from typing import Sequence, Tuple, List\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "git_folder = '/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/Inverse_Folding'\n",
    "esm_folder = '/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/esm/'\n",
    "sys.path.insert(1, os.path.join(git_folder, 'model'))\n",
    "sys.path.insert(1, os.path.join(git_folder, 'util'))\n",
    "\n",
    "sys.path.insert(1, esm_folder)\n",
    "import esm\n",
    "#from esm.inverse_folding import util\n",
    "import esm.pretrained as pretrained\n",
    "\n",
    "## I import this to try to get deeper on the sampling perplexity\n",
    "from esm.inverse_folding.features import DihedralFeatures\n",
    "from esm.inverse_folding.gvp_encoder import GVPEncoder\n",
    "from esm.inverse_folding.gvp_utils import unflatten_graph\n",
    "from esm.inverse_folding.gvp_transformer_encoder import GVPTransformerEncoder\n",
    "from esm.inverse_folding.transformer_decoder import TransformerDecoder\n",
    "from esm.inverse_folding.util import rotate, CoordBatchConverter \n",
    "\n",
    "\n",
    "#### Code for model with PLL ########\n",
    "#from potts_decoder import PottsDecoder\n",
    "#### Code for model with NCE ##########\n",
    "from potts_decoder import PottsDecoder\n",
    "from ioutils import read_fasta, read_encodings\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyhmmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_structure(fpath, chain=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fpath: filepath to either pdb or cif file\n",
    "        chain: the chain id or list of chain ids to load\n",
    "    Returns:\n",
    "        biotite.structure.AtomArray\n",
    "    \"\"\"\n",
    "    with open(fpath) as fin:\n",
    "        pdbf = pdb.PDBFile.read(fin)\n",
    "    structure = pdb.get_structure(pdbf, model=1)\n",
    "    bbmask = filter_backbone(structure)\n",
    "    structure = structure[bbmask]\n",
    "    all_chains = get_chains(structure)\n",
    "    if len(all_chains) == 0:\n",
    "        raise ValueError('No chains found in the input file.')\n",
    "    if chain is None:\n",
    "        chain_ids = all_chains\n",
    "    elif isinstance(chain, list):\n",
    "        chain_ids = chain\n",
    "    else:\n",
    "        chain_ids = [chain] \n",
    "    for chain in chain_ids:\n",
    "        if chain not in all_chains:\n",
    "            raise ValueError(f'Chain {chain} not found in input file')\n",
    "    chain_filter = [a.chain_id in chain_ids for a in structure]\n",
    "    structure = structure[chain_filter]\n",
    "    return structure\n",
    "\n",
    "def extract_coords_from_structure(structure: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        structure: An instance of biotite AtomArray\n",
    "    Returns:\n",
    "        Tuple (coords, seq)\n",
    "            - coords is an L x 3 x 3 array for N, CA, C coordinates\n",
    "            - seq is the extracted sequence\n",
    "    \"\"\"\n",
    "    coords = get_atom_coords_residuewise([\"N\", \"CA\", \"C\"], structure)\n",
    "    residue_identities = get_residues(structure)[1]\n",
    "    seq = ''.join([ProteinSequence.convert_letter_3to1(r) for r in residue_identities])\n",
    "    return coords, seq\n",
    "\n",
    "def get_atom_coords_residuewise(atoms: List[str], struct: biotite.structure.AtomArray):\n",
    "    \"\"\"\n",
    "    Example for atoms argument: [\"N\", \"CA\", \"C\"]\n",
    "    \"\"\"\n",
    "    def filterfn(s, axis=None):\n",
    "        filters = np.stack([s.atom_name == name for name in atoms], axis=1)\n",
    "        sum = filters.sum(0)\n",
    "        if not np.all(sum <= np.ones(filters.shape[1])):\n",
    "            raise RuntimeError(\"structure has multiple atoms with same name\")\n",
    "        index = filters.argmax(0)\n",
    "        coords = s[index].coord\n",
    "        coords[sum == 0] = float(\"nan\")\n",
    "        return coords\n",
    "\n",
    "    return biotite.structure.apply_residue_wise(struct, struct, filterfn)\n",
    "\n",
    "def get_loss_new(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    param_embeddings, fields = decoder.forward_new(encodings, padding_mask)\n",
    "    msas_embedded = embedding(msas)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll2(msas_embedded, param_embeddings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    \n",
    "    Q = torch.einsum('bkuia, buhia->bkhia', \n",
    "                param_embeddings.unsqueeze(2), param_embeddings.unsqueeze(1)).sum(axis=-1)\n",
    "    penalty = eta*(torch.sum(torch.sum(Q,axis=-1)**2) - torch.sum(Q**2) + torch.sum(fields**2))/B\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "\n",
    "def get_loss(decoder, inputs, eta):\n",
    "    \"\"\"eta is the multiplicative term in front of the penalized negative pseudo-log-likelihood\"\"\"\n",
    "    msas, encodings, padding_mask  = [input.to(device) for input in inputs]\n",
    "    B, M, N = msas.shape\n",
    "    #print(f\"encodings' shape{encodings.shape}, padding mask:{padding_mask.shape}\")\n",
    "    couplings, fields = decoder(encodings, padding_mask)\n",
    "\n",
    "    # embed and reshape to (B, M, N*q)\n",
    "    msas_embedded = embedding(msas).view(B, M, -1)\n",
    "\n",
    "    # get npll\n",
    "    npll = get_npll(msas_embedded, couplings, fields, N, q)\n",
    "    padding_mask_inv = (~padding_mask)\n",
    "\n",
    "    # multiply with the padding mask to filter non-existing residues (this is probably not necessary)       \n",
    "    npll = npll * padding_mask_inv.unsqueeze(1)\n",
    "    penalty = eta*(torch.sum(couplings**2) + torch.sum(fields**2))/B\n",
    "\n",
    "    # the padding mask does not contain the msa dimension so we need to multiply by M\n",
    "    npll_mean = torch.sum(npll) / (M * torch.sum(padding_mask_inv))\n",
    "    loss_penalty = npll_mean + penalty\n",
    "    return loss_penalty, npll_mean.item() \n",
    "    #return loss_penalty\n",
    "\n",
    "def get_loss_loader(decoder, loader, eta):\n",
    "\n",
    "    decoder.eval()\n",
    "    losses = 0\n",
    "    iterator = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs in loader:\n",
    "            iterator+=1\n",
    "            _, npll = get_loss_new(decoder, inputs, eta) \n",
    "            losses+=npll\n",
    "    \n",
    "    return losses/iterator\n",
    "\n",
    "def compute_covariance(msa, q):\n",
    "    \"\"\"\n",
    "    Compute covariance matrix of a given MSA having q different amino acids\n",
    "    \"\"\"\n",
    "    M, N = msa.shape\n",
    "\n",
    "    # One hot encode classes and reshape to create data matrix\n",
    "    D = torch.flatten(one_hot(msa, num_classes=q), start_dim=1).to(torch.float32)\n",
    "\n",
    "    # Remove one amino acid\n",
    "    D = D.view(M, N, q)[:, :, :q-1].flatten(1)\n",
    "\n",
    "    # Compute bivariate frequencies\n",
    "    bivariate_freqs = D.T @ D / M\n",
    "    \n",
    "    # Compute product of univariate frequencies\n",
    "    univariate_freqs = torch.diagonal(bivariate_freqs).view(N*(q-1), 1) @ torch.diagonal(bivariate_freqs).view(1, N*(q-1))\n",
    "\n",
    "    return bivariate_freqs - univariate_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "mutational_dir = '/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/Mutational_Data'\n",
    "msas_folder = '/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/Mutational_Data/alphafold_results_wildtype'\n",
    "\n",
    "protein_original_DMS = 'YAP1_HUMAN_1_b0.5.a2m.wildtype.fasta'\n",
    "structure_name = 'YAP1_HUMAN_1_b0.5.a2m_unrelaxed_rank_1_model_5.pdb'\n",
    "\n",
    "folder_fasta = os.path.join(mutational_dir, 'alignments')\n",
    "native_path = os.path.join(folder_fasta, protein_original_DMS)\n",
    "structure_folder = os.path.join(mutational_dir, 'alphafold_results_wildtype')\n",
    "structure_path = os.path.join(structure_folder, structure_name)\n",
    "\n",
    "num_seq = read_fasta(native_path, mutated_exp=True)\n",
    "structure =  load_structure(structure_path)\n",
    "coords, native_seq = extract_coords_from_structure(structure)\n",
    "coords=torch.tensor(coords).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet='ACDEFGHIKLMNPQRSTVWY-'\n",
    "default_index = alphabet.index('-')\n",
    "aa_index = defaultdict(lambda: default_index, {alphabet[i]: i for i in range(len(alphabet))})\n",
    "aa_index_inv = dict(map(reversed, aa_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "fastapath = \"/media/luchinoprince/b1715ef3-045d-4bdf-b216-c211472fb5a2/Data/InverseFolding/Mutational_Data/alphafold_results_wildtype/MSAS_new/YAP1_HUMAN_1_b0.5.a2m.a3m\"\n",
    "with open(fastapath, mode=\"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = lines[1:]\n",
    "\n",
    "msa_true = []\n",
    "for line in range(len(lines)):\n",
    "    if line%2 == 0:\n",
    "        ## Take the end of sequence file\n",
    "        seq_str = lines[line]#[0:-1]\n",
    "        seq_num = []\n",
    "        for char in seq_str:\n",
    "            if char != '\\n':\n",
    "                seq_num.append(aa_index[char])\n",
    "        if len(seq_num) == 36:\n",
    "            msa_true.append(seq_num)\n",
    "        else:\n",
    "            print(\"failure\")\n",
    "\n",
    "msa_true = torch.tensor(msa_true)\n",
    "cov_true = compute_covariance(msa_true[::,::], q=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyhmmer.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyhmmer.easel.Alphabet.amino()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_hmm = pyhmmer.easel.Alphabet.amino()\n",
    "alphabet_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyhmmer.easel.MSAFile(fastapath, digital=True, alphabet=alphabet_hmm) as msa_file:\n",
    "    msa = msa_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14484"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msa.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa.name = b\"YAP-HUMAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mpyhmmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplan7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fast'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mweighting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meffective_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprior_scheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msymfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfragthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.62\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mesigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.62\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEmL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEmN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEvL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEvN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEfL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEfN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mEft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpopen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpextend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwindow_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A factory for constructing new HMMs from raw sequences.\n",
      "\n",
      "Attributes:\n",
      "    alphabet (`~pyhmmer.easel.Alphabet`): The alphabet the builder is\n",
      "        configured to use to convert sequences to HMMs.\n",
      "    randomness (`~pyhmmer.easel.Randomness`): The random number generator\n",
      "        being used by the builder.\n",
      "    score_matrix (`str`): The name of the substitution matrix used to\n",
      "        build HMMs for single sequences.\n",
      "    popen (`float`): The *gap open* probability to use when building\n",
      "        HMMs from single sequences.\n",
      "    pextend (`float`): The *gap extend* probability to use when building\n",
      "        HMMs from single sequences.\n",
      "\n",
      ".. versionadded:: 0.2.0\n",
      "\n",
      ".. versionchanged:: 0.4.2\n",
      "   Added the `~Builder.randomness` attribute.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Create a new sequence builder with the given configuration.\n",
      "\n",
      "Arguments:\n",
      "    alphabet (`~pyhmmer.easel.Alphabet`): The alphabet the builder\n",
      "        expects the sequences to be in.\n",
      "\n",
      "Keyword Arguments:\n",
      "    architecture (`str`): The algorithm to use to determine the\n",
      "        model architecture, either ``\"fast\"`` (the default), or\n",
      "        ``\"hand\"``.\n",
      "    weighting (`str`): The algorithm to use for relative sequence\n",
      "        weighting, either ``\"pb\"`` (the default), ``\"gsc\"``,\n",
      "        ``\"blosum\"``, ``\"none\"``, or ``\"given\"``.\n",
      "    effective_number (`str`, `int`, or `float`): The algorithm to\n",
      "        use to determine the effective sequence number, either\n",
      "        ``\"entropy\"`` (the default), ``\"exp\"``, ``\"clust\"``, ``\"none\"``.\n",
      "        A number can also be given directly to set the effective\n",
      "        sequence number manually.\n",
      "    prior_scheme (`str`): The choice of mixture Dirichlet prior when\n",
      "        parameterizing  from counts, either ``\"laplace\"``\n",
      "        or ``\"alphabet\"`` (the default).\n",
      "    symfrac (`float`): The residue occurrence threshold for fast\n",
      "        architecture determination.\n",
      "    fragthresh (`float`): A threshold such that a sequence is called\n",
      "        a fragment when :math:`L \\le fragthresh         imes alen`.\n",
      "    wid (`double`): The percent identity threshold for BLOSUM relative\n",
      "        weighting.\n",
      "    esigma (`float`): The minimum total relative entropy parameter\n",
      "        for effective number entropy weights.\n",
      "    eid (`float`): The percent identity threshold for effective\n",
      "        number clustering.\n",
      "    EmL (`int`): The length of sequences generated for MSV fitting.\n",
      "    EmN (`int`): The number of sequences generated for MSV fitting.\n",
      "    EvL (`int`): The lenght of sequences generated for Viterbi fitting.\n",
      "    EvN (`int`): The number of sequences generated for Viterbi fitting.\n",
      "    EfL (`int`): The lenght of sequences generated for Forward fitting.\n",
      "    EfN (`int`): The number of sequences generated for Forward fitting.\n",
      "    Eft (`float`): The tail mass used for Forward fitting.\n",
      "    seed (`int`): The seed to use to initialize the internal random\n",
      "        number generator. If ``0`` is given, an arbitrary seed will\n",
      "        be chosen based on the current time.\n",
      "    ere (`double`, optional): The relative entropy target for effective\n",
      "        number weighting, or `None`.\n",
      "    popen (`float`, optional): The *gap open* probability to use\n",
      "        when building HMMs from single sequences. The default value\n",
      "        depends on the alphabet: *0.02* for proteins,\n",
      "        *0.03125* for nucleotides.\n",
      "    pextend (`float`, optional): The *gap extend* probability to use\n",
      "        when building HMMs from single sequences. Default depends on\n",
      "        the alphabet: *0.4* for proteins, *0.75* for nucleotides.\n",
      "    score_matrix (`str`, optional): The name of the score matrix to\n",
      "        use when building HMMs from single sequences. The only\n",
      "        allowed value for nucleotide alphabets is *DNA1*. For\n",
      "        proteins, *PAM30*, *PAM70*, *PAM120*, *PAM240*, *BLOSUM45*,\n",
      "        *BLOSUM50*, *BLOSUM62* (the default), *BLOSUM80* or\n",
      "        *BLOSUM90* can be used.\n",
      "    window_length (`float`, optional): The window length for\n",
      "        nucleotide sequences, essentially the max expected hit length.\n",
      "        *If given, takes precedence over* ``window_beta``.\n",
      "    window_beta (`float`, optional): The tail mass at which window\n",
      "        length is determined for nucleotide sequences.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.9/site-packages/pyhmmer/plan7.cpython-39-x86_64-linux-gnu.so\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?pyhmmer.plan7.Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = pyhmmer.plan7.Builder(alphabet_hmm, symfrac=0.0)\n",
    "background = pyhmmer.plan7.Background(alphabet_hmm)\n",
    "hmm, _, _ = builder.build_msa(msa, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daPLPpGWeeavdpdGrvYYyNheTgettWedPreA'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/esm/esm/pretrained.py:174: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n",
      "/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/esm/esm/inverse_folding/util.py:246: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  F.pad(torch.tensor(cd), (0, 0, 0, 0, 1, 1), value=np.inf)\n"
     ]
    }
   ],
   "source": [
    "model, alphabet_esm = pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "model.eval();\n",
    "model.to(device)\n",
    "rep = esm.inverse_folding.util.get_encoder_output(model, alphabet_esm, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/luchinoprince/Dropbox/Old_OneDrive/Phd/Second_year/research/Feinauer/esm/esm/inverse_folding/gvp_transformer.py'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample.__globals__['__file__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are at sample 49 out of 50\r"
     ]
    }
   ],
   "source": [
    "samples_esm = []\n",
    "samples_str = []\n",
    "samples_hmm = []\n",
    "for attempt in range(50):\n",
    "    print(f\"We are at sample {attempt} out of {50}\", end=\"\\r\")\n",
    "    sample = model.sample(coords, temperature=1.0)\n",
    "    name = f\"sample{attempt}\".encode('ASCII')\n",
    "    sample_dig = pyhmmer.easel.TextSequence(name = name, sequence=sample).digitize(alphabet_hmm)\n",
    "    samples_hmm.append(sample_dig)\n",
    "    samples_str.append(sample)\n",
    "    seq_num = []\n",
    "    for char in sample:\n",
    "        seq_num.append(aa_index[char])\n",
    "    samples_esm.append(seq_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msa_hmm  = pyhmmer.easel.TextMSA(name=b\"msa\", sequences=samples_hmm)\n",
    "#msa_d = msa_hmm.digitize(alphabet_hmm)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_aligned = pyhmmer.hmmer.hmmalign(hmm, samples_hmm, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msa_aligned.alignment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MKSMPEGYLAISDNEGNRQYYNTTTDQISIADPRQE'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'sample0'   ---MPEGYLAISDNEGNRQYYNTTTDQISIADPRQ- ...\n",
      "b'sample1'   ERPLPEGYTAVSTAEGKTLFIDNNTKQATGIDPR-- ...\n",
      "b'sample2'   --ALPKGWKKATTASGKQVYYDSKKATVTSKDPR-- ...\n",
      "b'sample3'   --PLPDGYVEQYTKHGTKIYFDTETQTVTYTDPREA ...\n",
      "b'sample4'   ---LPDGYVEITTLRGRLLYFDSSRRKVSLVDPR-- ...\n",
      "b'sample5'   ---NPIGWIQTNTDDGTVVFYNSERDMVTRSDPR-- ...\n",
      "b'sample6'   ----PIGWVEESDEEGVQFYWNTVQNTRSHEDPR-- ...\n",
      "b'sample7'   ---LPAGWVAVKNDSGETFFFDSKTNTQSWEDPRQ- ...\n",
      "b'sample8'   ---MPEGWRAHDNGNGTKFYFDGNNNTSSWFDPR-- ...\n",
      "b'sample9'   ----PFGWTVVYTKTGKSLYVDKNQNTISGVDPR-- ...\n",
      "b'sample10'   ---MPAGWLRLFTDQGDQIYFDMNTKTTTWQDPRQ- ...\n",
      "b'sample11'   -TPLPEGYVEIYDGAGRKHYFDDNTKTATKDDPRD- ...\n",
      "b'sample12'   --ELPDGFYQWHNSEGETWYYDTTTETSTKEDPR-- ...\n",
      "b'sample13'   ----PPGWVDRVAPTGEKFFYDSRWGRETWTDPRQ- ...\n",
      "b'sample14'   ----PFGWTEIYTDTGTLLYYNGVTHKASSVDPR-- ...\n",
      "b'sample15'   ----PCGYQSRKSSSGQRFYYDANTQTSTWIDPRD- ...\n",
      "b'sample16'   ----PTGWRILHTADGTAVYFDQSAFIVSRDDPRQ- ...\n",
      "b'sample17'   --PPPSGWKRVYDKSGKRHWYNSNTNTTSWYDPRE- ...\n",
      "b'sample18'   -IPMPAGWVQCTLATGTVVFYNQKTQTISSEDPR-- ...\n",
      "b'sample19'   ----PAGYDRMYSKAGTAYYYNRTTDQTSWVDPR-- ...\n",
      "b'sample20'   ----PAGWKELKTSKGKTVFYNTITHTISFADPR-- ...\n",
      "b'sample21'   --PLPEGYMEISDDLGQKLYYDDRTGRVSRTDPR-- ...\n",
      "b'sample22'   ----PEGWVEVRSDKGEKYFVNMRTHEKTWVDPRE- ...\n",
      "b'sample23'   ---------TLFDKRGVRYYYNSRTNQTSYEDPR-- ...\n",
      "b'sample24'   ----PEGWVSLTDNDGNKVYYNRNTENWSGLNPR-- ...\n",
      "b'sample25'   ----PKGWRSVYSTNGKRYYVNRLKKQASSKDPR-- ...\n",
      "b'sample26'   ---LPEGWLAVFEATGGKLYYNSNTNRVTFEDPRE- ...\n",
      "b'sample27'   ---LPDGWTQVQSSGGKTWYYNSRENTRTWQNPR-- ...\n",
      "b'sample28'   ----PEGYEIRYDESGVRYYFNSNTEETTWDDPR-- ...\n",
      "b'sample29'   --PPPKGWTVQHDSSGNEYYYDANTNTRTWTDPR-- ...\n",
      "b'sample30'   ---LPDGWVQQSDAHGQTYYYDMNTQTTSWDDPREA ...\n",
      "b'sample31'   ----PAGYEELKTKVGDTYYYNSKTNEISWKDPRE- ...\n",
      "b'sample32'   --PGPRGYETRYSNEGRPYYWNSNTQTTTWEDPRE- ...\n",
      "b'sample33'   ---MPEGWEEYVSENGETYYFNALTRTVSWEDPR-- ...\n",
      "b'sample34'   EEPLPDGFEAVKTGSGKTIYIDHQTNTVTSTDPRT- ...\n",
      "b'sample35'   ---MPAGYMKVKTANGRTVYFDANTSTITYNDPR-- ...\n",
      "b'sample36'   --PPPEGYEDLFSPEGKKFYYNNTLQEATWEDPRE- ...\n",
      "b'sample37'   --PQPLGWVSTEGANGETQYYDTRNQTLSYRDPRQ- ...\n",
      "b'sample38'   ---LPRGWKELRWSNGARYFYDANADTTTSQDPR-- ...\n",
      "b'sample39'   ----PLGWEAKYTSDGMRIFYDKNTQTWTWLDPR-- ...\n",
      "b'sample40'   --PLPEGWEEVKTRKGTTVYFDKKTMKVTTVDPRE- ...\n",
      "b'sample41'   ----PAGWVVRKTKAGKRIYWNKNTETWTWKDPRD- ...\n",
      "b'sample42'   --PLPDGFEAAYTENGKRVYYDTSTSTWTYDDPRQ- ...\n",
      "b'sample43'   ---LPEGWSGMRDDGGHQFYYNSTTAETTWEDPR-- ...\n",
      "b'sample44'   --PMPEGYEETQDSAGRVYYYNAKTKQKTWNDPR-- ...\n",
      "b'sample45'   --ELPEGWESKVTDDGERVFFDANTMTWTYHDPRE- ...\n",
      "b'sample46'   ---MPDGWLSVKSKTGQTLYLDKNTNVVSSQDPR-- ...\n",
      "b'sample47'   NVPLPEGFSEALTTAGEVYYYNSNTNTLTTKDPR-- ...\n",
      "b'sample48'   --PPPAGFQVQYDKDGKEFYYDKATDTWTYLNPL-- ...\n",
      "b'sample49'   --PLPEGYEEKTTKKGKTIYYDKGTDTWSSRDPR-- ...\n"
     ]
    }
   ],
   "source": [
    "for name, aligned in zip(msa_aligned.names, msa_aligned.alignment):\n",
    "    print(name, \" \", aligned[:48], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        getset_descriptor\n",
      "\u001b[0;31mString form:\u001b[0m <attribute 'alignment' of 'pyhmmer.easel.TextMSA' objects>\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "`tuple` of `str`: A view of the aligned sequences as strings.\n",
      "\n",
      "This property gives access to the aligned sequences, including gap\n",
      "characters, so that they can be displayed or processed column by\n",
      "column.\n",
      "\n",
      "Examples:\n",
      "    Use `TextMSA.alignment` to display an alignment in text\n",
      "    format::\n",
      "\n",
      "        >>> for name, aligned in zip(luxc.names, luxc.alignment):\n",
      "        ...     print(name, \" \", aligned[:40], \"...\")\n",
      "        b'Q9KV99.1'   LANQPLEAILGLINEARKSWSST------------PELDP ...\n",
      "        b'Q2WLE3.1'   IYSYPSEAMIEIINEYSKILCSD------------RKFLS ...\n",
      "        b'Q97GS8.1'   VHDIKTEETIDLLDRCAKLWLDDNYSKK--HIETLAQITN ...\n",
      "        b'Q3WCI9.1'   LLNVPLKEIIDFLVETGERIRDPRNTFMQDCIDRMAGTHV ...\n",
      "        b'P08639.1'   LNDLNINNIINFLYTTGQRWKSEEYSRRRAYIRSLITYLG ...\n",
      "        ...\n",
      "\n",
      "    Use the splat operator (*) in combination with the `zip`\n",
      "    builtin to iterate over the columns of an alignment:\n",
      "\n",
      "        >>> for idx, col in enumerate(zip(*luxc.alignment)):\n",
      "        ...     print(idx+1, col)\n",
      "        1 ('L', 'I', 'V', 'L', 'L', ...)\n",
      "        2 ('A', 'Y', 'H', 'L', 'N', ...)\n",
      "        ...\n",
      "\n",
      ".. versionadded:: 0.4.8"
     ]
    }
   ],
   "source": [
    "?pyhmmer.easel.TextMSA.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MPEGYLAISDNEGNRQYYNTTTDQISIADPRQ'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## THis does not make a lot of sense\n",
    "msa_aligned.sequences[0].sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERPLPEGYTAVSTAEGKTLFIDNNTKQATGIDPRAK'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"---MPEGYLAISDNEGNRQYYNTTTDQISIADPRQ-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACDEFGHIKLMNPQRSTVWY-BJZOUX*~'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet_hmm.symbols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WORKED OUT HOW TO USE THE LIBRARY, THE PROBLEM IS THAT IT DOES NOT GIVE BACK GAPS WHEN ACCESSING SEQUENCES, AND WE WANT ALSO THOSE SINCE WE USE THEM TO CONSTRUCT THE PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
